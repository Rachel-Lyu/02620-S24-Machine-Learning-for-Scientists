{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruiqil/miniconda3/envs/struct/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def read_csv_data(file_path):\n",
    "    data = defaultdict(list)\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            group_id = row['Group ID of project being reviewed']\n",
    "            data[group_id].append(row)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_numeric_features(data):\n",
    "    averages = {}\n",
    "    numeric_fields = [\n",
    "        'How clearly the problem is formulated? Select in a range from 1 to 5.',\n",
    "        'How appropriate is the dataset choice? Select in range from 1 to 5.',\n",
    "        'How feasible are the proposed methods? Select in a range from 1 to 5.',\n",
    "        'How many methods are described? Select between 0 to 3. If more than 3 is described, select 3.',\n",
    "        'Are the three methods properly described? Select in a range from 1 to 5. Select in a range between 1 to 5.',\n",
    "        'How appropriate is the proposed evaluation metrics to evaluate the methods? Select in a range between 0 to 5. Select 0 if evaluation criteria is not mentioned.',\n",
    "        'How many related works are discussed? Select in a range from 0 to 2. If more than 2 related works are discussed, select 2.',\n",
    "        'How appropriate is the choice of related works or how relevant they are to the main problem? Select in a range between 1 to 5.'\n",
    "    ]\n",
    "    \n",
    "    for group_id, reviews in data.items():\n",
    "        group_averages = {field: 0 for field in numeric_fields}\n",
    "        for review in reviews:\n",
    "            for field in numeric_fields:\n",
    "                group_averages[field] += int(review[field])\n",
    "        group_averages = {field: value / len(reviews) for field, value in group_averages.items()}\n",
    "        averages[group_id] = group_averages\n",
    "    \n",
    "    return averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_t5_model(model_name=\"t5-large\"):\n",
    "    \"\"\"\n",
    "    Initializes the T5 model and tokenizer with the specified model name.\n",
    "    \"\"\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def summarize_text(text, tokenizer, model, max_length=60, min_length=10):\n",
    "    \"\"\"\n",
    "    Generates a summary for the provided text using the T5 model.\n",
    "    \"\"\"\n",
    "    # Prefixing the text with \"summarize: \" to indicate summarization task\n",
    "    input_text = \"summarize: \" + text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generating the summary\n",
    "    summary_ids = model.generate(input_ids, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def summarize_open_questions(data):\n",
    "    summaries = {}\n",
    "    open_questions = [\n",
    "        'Provide brief justification on your rating for \"How clearly the problem is formulated? \"',\n",
    "        'Provide brief justification on your rating for \"How appropriate is the dataset choice? \"',\n",
    "        'Provide brief justification on your rating for \"How feasible are the proposed methods?\"',\n",
    "        'Provide brief justification on your rating for \"Are the three methods properly described?\"',\n",
    "        'How appropriate are the proposed methods for the problem being solved? Briefly explain your decision for each model.',\n",
    "        'Provide brief justification on your rating for \"How appropriate is the proposed evaluation metrics to evaluate the methods?\"',\n",
    "        'Provide brief justification on your rating for \"How appropriate is the choice of related works or how relevant they are to the main problem?\"',\n",
    "        'Briefly mention the overall strengths of the proposal.',\n",
    "        'Briefly mention the overall weaknesses of the proposal.'\n",
    "    ]\n",
    "    \n",
    "    for group_id, reviews in data.items():\n",
    "        group_summaries = {}\n",
    "        for question in open_questions:\n",
    "            # Concatenate all responses for the question\n",
    "            concatenated_responses = \" \".join(review[question] for review in reviews if review[question].strip())\n",
    "            \n",
    "            # Summarize the concatenated responses if they are not empty\n",
    "            if concatenated_responses:\n",
    "                summarized_response = summarize_text(concatenated_responses, tokenizer, model)\n",
    "                print('Before: ', concatenated_responses)\n",
    "                group_summaries[question] = summarized_response\n",
    "                print('After: ', summarized_response)\n",
    "            else:\n",
    "                group_summaries[question] = \"No responses provided.\"\n",
    "                \n",
    "        summaries[group_id] = group_summaries\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "def export_results(file_path, averages, summaries):\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Group ID of project being reviewed'] + list(averages[list(averages.keys())[0]].keys()) + list(summaries[list(summaries.keys())[0]].keys())\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for group_id in averages:\n",
    "            row = {'Group ID of project being reviewed': group_id, **averages[group_id], **summaries[group_id]}\n",
    "            writer.writerow(row)\n",
    "            \n",
    "tokenizer, model = initialize_t5_model()\n",
    "\n",
    "# Main execution\n",
    "file_path = 'ProjReview.csv'\n",
    "output_path = 'ProjReview_results.csv'\n",
    "data = read_csv_data(file_path)\n",
    "averages = average_numeric_features(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  Title and the first sentence of the first paragraph all reveals the studied problem clearly. The purpose for training the model for specific dataset and the real-life significance for doing so is clearly identified and very convincing. The problem is quite easy understanding in its title and first paragraph. The problem is to use x-ray images for disease prediction. You explained how you plan to solve the problem, but the problem itself was not stated. The proposal talks about the model and its potential uses if successful, but doesn't clearly address current / technical problem(s) that are being faced.\n",
      "After:  the problem is to use x-ray images for disease prediction. you explained how you plan to solve the problem, but the problem itself was not stated.\n",
      "Before:  There have been a lot of related works using this data, and I think it is reliable and workable. The data set chosen is very match with the purpose of the paper and has been verified with other research work. This somehow ensures the feasibiliy of using this dataset for training. the dataset has clear advantages well-annotated and relatively large, also already tested by other models It was used for other methods for similar purpose. The dataset looks fine. Data set features not mentioned. The proposal uses existing data that have been used for other research, but doesn’t mention the features of the dataset(quality/organ/disease/patient) in the proposal.\n",
      "After:  the data set chosen is very match with the purpose of the paper and has been verified with other research work. the dataset has clear advantages well-annotated and relatively large, also already tested by other methods for similar purpose.\n",
      "Before:  The listed method are very basic, and they have backup plans. The idea for using CLIP is a good way for transforming image data to binary descriptive data that can be processed for ML models. It will be good if more detail can be included for how this works and what are the output data we are expecting from CLIP. In addition a bit concern is that the cited paper for similar work all use neural network and deeplearningm so not sure whether the similar pipeline can works well on ML algorithsms. classfication methods should be utilized when the result is binary Not exactly sure how CLIP works, but I suppose it gives you list of features where you could do regression / decision tree / SVM. From the proposal it seems that the confidence in the methods is low. \"Although this project is somewhat risky, itwill give insight into image classification models.\" The proposal mentions the following line, which shows that there is uncertainty.\n",
      "After:  the idea for using CLIP is a good way for transforming image data to binary descriptive data that can be processed for ML models. from the proposal it seems that the confidence in the methods is low.\n",
      "Before:  The mentioned methods can do classification works as they want. The models are properly described with reason why they are chosen for this project. Backup plans are also proposed which is good to include in the proposal. the three classfication methods are mentioned but not well described No justifications for choice of methods. All the methods are clearly described in their own section. The proposal mention the 3 methods, but doesn't clearly describe why/how they will be used.\n",
      "After:  the mentioned methods can do classification works as they want. backup plans are also proposed which is good to include in the proposal. the three classfication methods are mentioned but not well described.\n",
      "Before:  SVM, random forest, and logistic regression are basic method used for classification. The purpose is to do classification of disease sample based on image feature, and if the data transformed from CLIP is representitive enough to show the difference of certain pathological features those method (Random Forest, SVM) should be very workable for such classification. However not very sure whether LIneary regression works in the case as it is not a classification method but a regression method? classfication methods should be utilized when the result is binary As stated before, since the input is a list of feature value and output is classification, the proposed methods should work. the only methods that seems appropriate is binary classification. The papers you linked do not provide evidence that the other methods work. The proposed methods are being used to train a model which has images. As per my understanding, traditional ML models don't really work well when it comes to images.\n",
      "After:  SVM, random forest, and logistic regression are basic method used for classification. however not very sure whether logistic regression works in the case as it is not a classification method but a regression method? the only methods that seems appropriate is binary classification.\n",
      "Before:  They have listed the most commonly used evaluation metrics. Standard cross validation are applied and detailes are mentioned for how specifically the precision are evaluated and improved. accuracy, precision, and F1 score are appropriate for evalutation More description on how you want to tune using your validation set? Other than that it seems to be fine. The proposed evaluation metrics seem fine, how would regression be evaluated? The evaluation metrics to evaluate the methods look fine, but don't mention any previous/current scores for the accuracy that have been observed in the 'known relevant papers'.\n",
      "After:  the proposed evaluation metrics seem fine, but don't mention any previous/current scores for the accuracy that have been observed in the 'known relevant papers'.\n",
      "Before:  The related works even used the same dataset, so it is reasonable. The cited paper performs similar research on the question with same dataset, and act as a justification for the feasibility of the research question. Very convincing supplementary works to justify the proposed research project. the same data related analysis are clearly described They seem to be works related and have ML methods used to solve the same problem. Papers do not seem to support the proposed methods. The main works are related to images, but they do not mention any classical ML methods.\n",
      "After:  cited paper performs similar research on the question with same dataset. main works are related to images, but they do not mention any classical ML methods.\n",
      "Before:  The proposal covered everything that was graded in the feedback and answered them clearly and condensed. Very good job for explaining the research question and its significance, and supported by siumilar research to show the feasibility for the idea and the data chosen. Good job for examining the data at first and propose model based on the data. Reasonable choice of the models based on the classification purpose of the work. dataset and methods are clear and feasible The problem is clearly stated and seem to have a method to already generate features from image input. The problem seems challenging, and the model would be really great if successful. The proposal looks good overall with an ambitious goal, which if achieved would be a good model.\n",
      "After:  the proposal covered everything that was graded in the feedback and answered them clearly and condensed. the problem seems challenging, and the model would be really great if achieved.\n",
      "Before:  Maybe it can detailedly explain how each method works for this data and describe the method. The main concern when i read the proposal is the CLIP part, whether the output is representitive enough for chest x-ray images (as i saw samples on Open AI are more diversed and colorful, so might question whether it is clear enough to give a translation good enough to distinguish x-ray images). methods description not clear enough Not enough justification on method used. Also, more information on the CLIP would help since it appears to also function on other ML methods. You might want to take a look at how it process your input and potentially look for adversarial examples. I remember a previous ML method trained on tumor image identification which was later proven to identify rulers as tumors since they were often positioned next to a tumor in the training images. Methods seem unfeasible (papers do not mention them) except for binary classifcation. The writing of the proposal can be improved, and a clear problem statement along with mention of existing ML methods other than CNN and neural networks could have been included to solidify that this approach is a good one.\n",
      "After:  the main concern when i read the proposal is the CLIP part, whether the output is representitive enough for chest x-ray images. more information on the CLIP would help since it appears to also function on other ML methods.\n",
      "Before:  The problem seems well formulated, in that they have a clear goal, (Classification), and background for said goal. i thought the introduction to the problem was very well written, and it was clear exactly what issue you were trying to tackle, there was also very good description of the methods you plan on using The problem is clearly articulated, highlighting the connection between circadian rhythms, CRY1, and human health, setting a well-defined research objective​ Proposal clearly describes the importance of elucidating toxicity in the drug discovery process They clearly stated the traditional manner in which diseases are classified, mentioning the limitations of these methods. They then described how they will address this challenge through their methodology. They try to use molecular descriptors of molecules designed to target CRY1 to predict their toxicity. Stated clearly at the start. The proposal explains the problem statement well along with a brief introduction on the topic and why it is important.\n",
      "After:  the problem seems well formulated, in that they have a clear goal, (Classification) i thought the introduction to the problem was very well written. the proposal explains the problem statement well along with a brief introduction on the topic and why it is important.\n",
      "Before:  The dataset appears to be relevant to the goal, but there is no source to said dataset. Not sure how complex the problem might be but could potentially use larger dataset? Either way it does seem like a good dataset to use for the problem. Highly specific and relevant, ensuring direct applicability to the goal of enhancing drug discovery through toxicity classification Dataset seems appropriate - not exactly sure of the features in the dataset They explain how the techniques can be performed on that dataset. The dataset is described but no link or work cited was given to point to the data. Dataset seems relevant to the problem The proposal aim to work on already worked on data, but hasn't cited the source or the paper properly.\n",
      "After:  the dataset appears to be relevant to the goal, but there is no source to said dataset. not sure how complex the problem might be but could potentially use larger dataset?\n",
      "Before:  The proposed methods are feasible. Good overall choices for classifier methods so should perform pretty well. For feature elimination, recursive feature elimination may take a long time because of iteratively going through subsets of features (could do PCA instead?) The proposed methods are technically sound, appropriate for the dataset, and capable of addressing the classification task, albeit with potential resource demands​ Given that the methods align with the relevant literature, they seem to be feasible The methods can be implemented on the dataset. Appears to be reasonable to reduce the dimension of input (still, no data found). Decision tree and regression also seem reasonable for classification task. Group is replicating a model, so it is feasible. The proposal is confident about one model, but isn't sure how the other models would perform.\n",
      "After:  the proposed methods are technically sound, appropriate for the dataset, and capable of addressing the classification task. recursive feature elimination may take a long time because of iteratively going through subsets of features. the proposal is confident about one model, but is\n",
      "Before:  The three methods do appear to be properly described. very good descriptions on classifiers and recursive feature elimination and how you plan to use them! Well-chosen for the problem, utilizing advanced techniques that are expected to effectively handle the classification challenge​ A description for how each method will be implemented is given They provided an explanation of how they will implement each of the 3 methods. Well explained for each method. Although the third method seems to be just an enhanced decision tree, so not sure if that counted as a third method. All methods described well in methods section. The methods are properly described and the results are clearly mentioned from previous experiments. The group is also confident about the success of one method.\n",
      "After:  the methods do appear to be properly described. the group is also confident about the success of one method.\n",
      "Before:  I think logistic regression is the only method that seems slightly odd in its usage. I don't know if logistic regression to extract out important features is standard practice. It does seem like it would work. since it is a classification task, methods are definitely appropriate and should perform pretty well i think The proposed methods include dimensionality reduction, a decision tree classifier (DTC), and a gradient boosted classifier. These methods are well-explained and appropriate for handling the dataset's characteristics and the classification task. However, the technical complexity and computational demands of these methods, particularly gradient boosting, could present feasibility challenges, particularly in terms of computational resources and model tuning. Logistic regression - seems appropriate given numerical data and binary classification; DTC - seems appropriate given expected non-linearity of decision boundary; GB DTC - seems like an appropriate follow up to DTC to improve performance Logistic regression is appropriate for binary classification. The decision tree will be valuable when the variables are interrelated for binary classification. Gradient Boosting is built off of decision trees and can be utilized for binary classification. Since the output toxicity (binary classification), with numerical input, a logistic regression model seems to be a natural choice. A decision tree is also reasonable but need to decide threshold for each branch (sorry I still do not like trees, prefer regressions). All methods are appropriate since they are based on a paper. The proposed method seem appropriate for the following problem, especially since they are working/replicating an already performed research work.\n",
      "After:  the proposed methods include dimensionality reduction, a decision tree classifier (DTC), and a gradient boosted classifier. the technical complexity and computational demands of these methods, particularly gradient boosting, could present feasibility challenges. since the output toxicity (binary classification\n",
      "Before:  I think the evaluation metrics do appear to be appropriate besides logistic regression. evaluation metrics make sense considering you are doing binary classification Thoughtfully selected, focusing on minimizing false negatives, which is crucial in the context of identifying toxic compounds They mention they can use standard metrics given binary classification problem, they also address the unique need to penalize false negatives more harshly given the biological context of the problem They mentioned the evaluation metrics that we discussed in class and can compare them to verify their correctness. Good description and emphasis focusing on false negatives. Although the second half describing expectation for each method peformance should be in the method proposal section. The proposed evaluation metrics are fine since it is based on a paper. The proposal correctly uses F1 scores as a metric and also highlight the false negative rates and how they can work around it.\n",
      "After:  the proposed evaluation metrics do appear to be appropriate besides logistic regression. the proposal correctly uses f1 scores as a metric and also highlight the false negative rates and how they work around it.\n",
      "Before:  The related works are relevant to the choices being done for ML methods. The greedy gradient boosting paper does seem relevant to the attempt to optimize the decision forest learning method. Additionally, the other paper provides domain knowledge to the choice of dataset. approaches definitely seem valid and based in literature, so related works apply very well to main problem (could use citations for related works though) The choice and discussion of related works, including the structural design and classification of small molecules for circadian rhythm regulation and the methods for gradient boosting, are highly relevant. Their relevant works include the paper that introduced the dataset, so this seems extremely relevant, as well as a paper on gradient boosting, which is relevant given they plan to implement this They utilize the papers to better understand their machine learning techniques. Cannot determine for sure sine the works were not cited. But I suppose they are since the proposal is well written and they seem to know very well what they are talking about. Project is based on the paper. Though the proposal mentions only one paper, it try to replicate an existing relevant work , making it appropriate for the given problem.\n",
      "After:  related works are relevant to the choices being made for ML methods. proposal mentions only one paper, but it try to replicate an existing relevant work. related works apply very well to the main problem.\n",
      "Before:  The strengths of the proposal are its well formulated problem and goal. well-supported approaches, good descriptions of how approaches apply to problem, and good justification for evaluation criteria The problem is well-defined and highly relevant to human health. Overall, the proposal is very clearly written and clearly describes the problem this project is aiming to address. The strengths are that all the methods are described properly and a deep literature review has been conducted. Well stated problem and good insight into the output and focus on what error to look out for. Great proposal, everything was clear. The proposal is well written and clearly states the problems, methods, and the possible areas where there could be hurdles.\n",
      "After:  the proposal is very clearly written and clearly describes the problem this project is aiming to address. the strengths are that all the methods are described properly and a deep literature review has been conducted.\n",
      "Before:  One failure could be the failure to evaluate unsupervised methods for the dataset, which might be useful to visualize the dataset. Additionally, using logistic regression to perform feature selection might not give features that are actually representative for classifying the dataset. only weakness could be the dataset not having enough samples? but this may not be a problem depending on how complex the overall goal is The proposal could detail more about the computational resources required for the proposed methods, particularly gradient boosting, which can be resource-intensive. Only potential weakness is the proposal could include a better explanation of what the features of the each molecule are in the dataset There aren't any apparent weaknesses. Related work not cited. Please include a citation for the paper being used. The proposal fails to cite the paper properly and only focuses on one relevant research paper.\n",
      "After:  one failure could be the failure to evaluate unsupervised methods for the dataset. only weakness could be the dataset not having enough samples? proposal could detail more about the computational resources required for the proposed methods.\n",
      "Before:  The probelm is well articulated, classify disease status based on gene expression. I think overall the problem is formulated very well. Methods make sense considering you are doing multi-classification. For the NN model, could be useful to indicate which classes you intend on merging, and also could be difficult to implement depending on how complex your NN model is (unless you want to use keras or something just for this model, which if so, great!). The problem is clearly outlined, focusing on classifying COVID-19 severity using gene expression data, which is crucial for medical diagnosis and understanding gene functionality related to disease severity​ The problem statement clearly describes the dataset and what the team wants to classify, it also states why the problem is useful to solve There is a specific section explaining the reference paper, dataset, and the goals the group plans to accomplish. I believe the problem is formulated well, but I am unsure about how exactly severity is measured, which is necessary for a good classification and/or regression. It is formed and clearly states the methods and is structured well in trying to classify a patient's COVID-19 severity based on available gene expression data, which shall facilitate medical diagnosis. Additionally, it also sheds light on further investigation of some selected genes’ functionality pertaining to COVID-19 severity. The authors clearly define the objective of classifying disease severity using gene expression data, and their goal of understanding the role of certain genes in COVID-19 severity. Problem is stated clearly in its own section. The proposal aims at identifying the genes that get impacted due to COVID-19, and aims to identify the patient severity based on the expression values.\n",
      "After:  the problem is clearly outlined, focusing on classifying COVID-19 severity using gene expression data. the authors clearly define the objective of classifying disease severity using gene expression data, and their goal of understanding the role of certain genes in COVID-19 severity.\n",
      "Before:  The dataset fits the appropriate problem at hand. dataset is very appropriate, and the the problem that dataset is used to solve is clearly defined in literature. though a bit unclear on how gene expression profiles for sequenced cells can relate back to a COVID19 classification for a single patient The dataset, consisting of gene expression levels from 124 patients, is directly relevant and exceptionally suited for investigating the genetic contributions to COVID-19 severity, making it an excellent choice​ They explain how building a classifier from this dataset allows them to draw biological insight about covid-19 based on the important features The dataset is aligned well with the problem, and it seems to have appropriate and useful labels for classification and learning purposes. The data come from an already published paper and should therefore be of good quality. It has a good amount of data to run and test The authors seek to examine the expression data of 37412 genes across 124 patients, and make sure to state that they will use PCA to reduce the dimensions of the data. This dataset choice is appropriate and are well-suited to perform the clustering and neural network analyses the authors propose. Dataset is from a database and is relevant to the problem. The proposal aims to use relevant data to understand the patient severity for COVID-19 based on expression data\n",
      "After:  gene expression levels from 124 patients is directly relevant and exceptionally suited for investigating the genetic contributions to COVID-19 severity. data come from an already published paper and should therefore be of good quality. authors seek to examine the expression data of 37412 genes across 124 patients\n",
      "Before:  All of the methods are feasible. The multi-layer perceptron could be performed using one of the main learning packages. methods are definitely feasible for multi-class classification The proposed methods, including PCA for dimensionality reduction, K-means clustering, and neural network analysis, are feasible and appropriate given the data's nature. They are well-described and align with current bioinformatics practices Their methods seem feasible, as they clearly explained why and how they will implement and evaluate each method These are all appropriate methods for classification and seem feasible and reasonable to complete. PCA and k-means should be quite feasible with numpy, while neural networks can easily use preimplemented packages. They semme pretty feasible given the data and classification. PCA and K-means clustering seems appropriate for the classification that the group wants to test. The authors present PCA, K-means clustering, and neural networks to analyze the data; it is slightly unclear how they intend to merge the classes in the original dataset using the neural network. All methods seem fine, unsure if CNN can increase accuracy, might be unfeasible The proposal is feasible and should give satisfactory results, if they hypothesis is right.\n",
      "After:  the proposed methods, including PCA for dimensionality reduction, are feasible. PCA and k-means clustering should be quite feasible with numpy. neural networks can easily be used preimplemented packages.\n",
      "Before:  The three methods are described vaguely, at least in terms of PCA and k-means. pca is well described, nn is well described, for k-means clustering instead of visually determining could use inertia between clusters? The methodologies are clearly described, providing a solid understanding of how each will be applied to analyze the dataset, though some additional specifics on implementation details could enhance clarity Each method is clearly described and they also describe how the method will be evaluated and their reasoning behind choosing each method Each method has a short justification. Well explained, and the authors did well in describing how they would use each one. Each method explains why it is used and how it applies to the project. The authors clearly define the reasoning behind selecting each method (PCA, clustering, neural network) and the question that each method will answer when looking at the gene expression data. All methods are described well in \"Methodologies and Evaluations\" The proposal uses PCA for dimension reduction and, and K-means clustering for grouping the datasets. It also aims to use of neural network or MLP in the above case to get better accuracy.\n",
      "After:  the methodologies are clearly described, providing a solid understanding of how each will be applied. each method explains why it is used and how it applies to the project. the authors clearly define the reasoning behind selecting each method.\n",
      "Before:  These methods are appropriate for the problem being solved, two unsupervised, and one supervised method. PCA is a sensible option for feature elimination, k-means and NN is also very good considering you are working towards multi-class classification The chosen methods are highly suitable for the dataset and the problem, leveraging advanced analytical techniques to extract meaningful insights from complex genetic data PCA - seems appropriate given the high dimensionality of the dataset, allows for exploratory analysis before developing complex models; clustering - seems like an appropriate follow up to PCA to visualize mathematically separated the data is in PC space; NN - seems appropriate since they state they want to achieve high accuracy and avoid overfitting These are appropriate methods for classification. Somewhat appropriate - PCA and neural networks are suitable for dimensionality reduction and a good model, but I am unsure about how k-means fits into the equation - does the data already have severity labels, and if so, why would we need k-means? If not, what would we do with neural networks? For PCA, it is used to reduce its dimensionality and visualize 2 to 3 selected components afterward. This can be done to both the entire dataset and partial datasets based on gene labels or subject COVID-19 severity. For K-means, it can be used to cluster based on COVID severity. For Neural networking, would be used to measure the precision, accuracy, and F1-score on the testing set. The methods are mostly well-defined and appropriate; I am slightly unsure about the use of MLP, as if there are only 124 patients it may suffer from overfitting and be difficult to optimize. All methods seem appropriate for the problem. PCA is based on paper, Kmeans find a good method for clustering, and CNN will work fine, but it might not increase accuracy. All apart from MLP seems to be appropriate methods. The proposal doesn't mention about why any other ML methods are being used.\n",
      "After:  k-means and NN are also very good considering you are working towards multi-class classification. i am slightly unsure about the use of MLP as if there are only 124 patients it may suffer from overfitting and be difficult to optimize.\n",
      "Before:  It is hard ot evaluate the unsupervised methods at hand, but easy to objectively evaluate the multi-perceptron model. The proposed evaluation metrics are good, could be helpful to describe how the metrics extend to multiple classes. Though confusion matrix is definitely valid, could also do ROC/AUC? The evaluation strategy using accuracy, precision, recall, F1-score, and confusion matrix analysis is well-suited for assessing the classification models, particularly in the context of a medical diagnosis problem The evaluation methods seem to be standard across the three methods they proposed. They said they will use accuracy, recall, precision, and F1 score. These are appropriate methods for evaluating classification. Standard evaluation, not much else to add - though it may be worth considering custom metrics to adapt to the specific problem Everything is explained after the methods are listed. The authors clearly describe that they will use DBSCAN/objective values to identify clusters, and accuracy, precision, recall scores for MLP. All he proposed evaluation metrics are clear and fit the methods well. The proposal gives proper reasoning for the evaluation metrics for all the methods.\n",
      "After:  the proposed evaluation metrics are good, could be helpful to describe how the metrics extend to multiple classes. the authors clearly describe that they will use objective values to identify clusters, and accuracy, precision, recall scores for MLP.\n",
      "Before:  The related works appears to be directly relevant to the main problem. It provides domain knowledge of the problem at hand. Having a paper that suggests that this kind of analysis/learning can be done with this data would be nice. the problem is very clearly defined in the related works, though a bit unclear on how the sepsis paper [1] relates specifically to the overall issue being addressed The cited works provide a strong background and justification for the study, highlighting the significance of genetic markers in COVID-19 severity and the importance of such research in advancing personalized medicine They chose relevant works that address similar problems related to COVID-19 and blood based biomarkers for severity, also touches on the biological significance These works are either relevant to the background/literature review or the structure of their problem and goals for this project. The works describe the main features that go into the problem and the dataset, but again they do not discuss how severity is quantified. They are related in the sense that they mention how prior research detects harmful patterns in other diseases like sepsis that can be tested to classify the severity of COVID-19. The related works chosen are three well-cited papers examining mild to severe cases of COVID-19 infection and its relationship with biomarkers and gene expression levels in humans, in addition to the main paper used to formulate the project proposal. These works are extremely relevant to the problem of classifying disease severity given gene expression data. All papers are related to the problem The proposal aims to explore one of the studies (mentioned paper) while using datasets from an established database, to identify the key factors that determine the patient severity when it comes to COVID infection.\n",
      "After:  related works appear to be directly relevant to the main problem. proposal aims to explore one of the studies while using datasets from an established database, to identify key factors that determine patient severity when it comes to COVID infection.\n",
      "Before:  The strengths of the proposal is that the problem is clear, and straightforward to achieve in the time constraints. very good model selection, sensible data preprocessing and feature selection steps, clearly defined problem and measures towards a solution The proposal strongly connects a well-defined problem with an appropriate, data-driven methodological approach, ensuring relevance through a focused dataset on COVID-19 gene expression levels. It demonstrates a robust evaluation plan with multiple metrics, grounded in a thorough literature review that underscores the project's significance and potential impact Overall, the proposal provides a clear description of the problem they are trying to address and systematically describes each method they plan to implement The proposal is concise and clearly outlines their problem, methods, and evaluation. The proposal seems very straightforward to implement and uses a dataset from a well-established paper, making the proposed project straightforward. It also discusses the biological relevance behind the features of the data in the relevant works. It flows well together yet is separated into sections. The authors detail their thought process behind solving the problem extremely effectively, and it was very easy for me to follow along with their justifications for particular methods and why their particular dataset was interesting. It should be easy to get started on solving the problem of disease severity classification just given the proposal. Everything is clearly described The paper is well written, and highlights the past work, methods and properly.\n",
      "After:  strengths of the proposal is that the problem is clear, and straightforward to achieve in the time constraints. the proposal is concise and clearly outlines their problem, methods, and evaluation. the paper is well written, and highlights the past work and properly.\n",
      "Before:  One of the weaknesses of the proposal is that it does not clearly outline the unsupervised methods and what they achieve to aim beyond the superficial. potential issues when implementing NN model if not using existing library, also could outline more specifically how single-cell sequencing data relates back to an overall patient classification The proposal might benefit from further detailing the computational requirements and the specific challenges of dealing with high-dimensional data, including more explicit strategies for data preprocessing and feature selection. Additionally, a more comprehensive discussion on the practical implications of the findings and how they might be integrated into clinical settings could enhance its applicability Could describe how they plan to draw insight about selected genes that influence severity I think they could include more detail about the specific methods they chose and further describe computational efficiency. The main one for me is that I am unsure of how severity is measured - a good measure is necessary for any supervised ML model. I think it could have been more concise to fit within the one-page criteria. I believe the authors could have expanded on their ideas for evaluating the performance of each of their methods; for instance providing an example workflow or test dataset that they may have encountered in their preliminary analysis. It may also be good to mention how they intend to perform the visualization (in R/python etc.). More in depth analysis on CNN and why it can improve accuracy. Minor formatting issues. Apart from grammatical / formatting mistakes, the proposal could have been made a bit more concise and mention a bit more on the previous/existing work.\n",
      "After:  the proposal does not clearly outline the unsupervised methods and what they achieve to aim beyond the superficial. the main one for me is that i am unsure of how severity is measured - a good measure is necessary for any supervised ML model.\n",
      "Before:  The problem appears to be well defined. clear description of how dataset will be used, what models will be built, and how the resulting models can be utilized to solve real-world issues The problem is clearly formulated, aiming to enhance breast cancer diagnosis by evaluating the efficacy of various machine learning models on the Wisconsin Breast Cancer Diagnostic Data Set, emphasizing the importance of accurately classifying tumors as malignant or benign They describe the dataset and what they want to predict from the dataset, as well as outline why its relevant The problem is clearly stated to be classification. Nothing much more to add - the problem of binary classification by various features is very well elaborated. The background information did a good job explaining why they wanted to optimize a current cancer classifier model The authors clearly define the need for an accurate classification model for identifying malignant cells within image datasets for the purposes of breast cancer diagnosis. it decompose the question into background and method, and it is formulated very clear what is the rational of proposing all these models? A good setting, emphasizes the need for active learning to improve diagnosis accuracy. It could have elaborated more on the specific challenges faced in the diagnosis process that active learning seeks to address, such as detailing the types of uncertainty most critical to resolve. The research question is clearly defined, a classifier based on nuclear morphology for breast cancer cells. But there is no discussion of the practical implications of the research.\n",
      "After:  the problem is clearly formulated, aiming to enhance breast cancer diagnosis. authors clearly define the need for an accurate classification model for identifying malignant cells. but there is no discussion of the practical implications of the research.\n",
      "Before:  The dataset fits the problem at hand. very good description of dataset, good mention of balanced characteristic, shape of dataset might help to determine if there are enough samples and what kinds of features there are he dataset choice is highly appropriate, focusing on breast tissue images and extracted cell nuclei features, which are crucial for distinguishing between malignant and benign conditions, directly supporting the project's goal​ Seems to be appropriate given its use in relevant works This is a well established dataset with many known features for classification. This dataset has been used in a variety of publications already, making it a good choice. It is difficult to find where the dataset was mentioned. They mentioned the dataset Analysis section on what they want to do based on the current research articles they mentioned. The authors state that they intend to examine images of fine needle aspirate from breast tissue, which is appropriate for their goal of classification given image analysis. it use WBCD dataset, which highly in line with the background and purpose of the research I don't fully understand what are \"continuous features\" discussed in their proposal, this dataset seems to be a classic labeled image dataset The chosen dataset is highly relevant for the study's goal, focusing on breast cancer diagnosis with features derived from cell nuclei images. The widely used public data sets are selected, the information sources are reliable and facilitate horizontal comparison.\n",
      "After:  focusing on breast tissue images and extracted cell nuclei features, which are crucial for distinguishing between malignant and benign conditions. well established dataset with many known features for classification, making it a good choice. authors state that they intend to examine images of fine needle aspir\n",
      "Before:  All of the methods are feasible, at least they can be implemetned in a fairly short amount of time. i think the proposed methods are good, SVM may be better than LR is data are not linearly separable, kNN might not be the best depending on how large the dataset is, and maybe should have some method for feature selection (PCA, recursive feature elimination, etc) The proposed methods, including kNN, Naive Bayes, logistic regression, and SVM classifiers, are feasible and well-suited for continuous features typical of the dataset. The approach of comparing these models to identify the optimal one for the dataset's specifics is practical and well-justified​ The methods proposed seem feasible given they are classical ML models and seem to have been previously applied and evaluated These four methods are appropriate for classification and seem feasible to implement. I believe that kNN can easily be implemented in numpy, and Naive Bayes can as well but may be harder. SVMs or logistic regression could easily be done with preexisting packages. They are unsure if LR or SVM ill out perform NB classifier, which is fine, but I think it should be more concrete on the direction of the methods. The authors propose k-nearest neighbor classification, Naive Bayes, logistic regression, and support vector machines as potential methods. The authors state they intend to compare the performances of all these classification methods. The authors make sure to clarify what metrics they intend to examine for each classifier (Euclidean/Manhattan distance, continuity, curvature), making these methods feasible for what they are proposing. it propose to use NB LR and SVM, which are all basic ML method, I think it can be done. maybe convolutional neural network shall have better performance? The methods are standard in the field and suitable for the dataset, making them generally feasible. An appropriate method was selected based on data characteristics (continuity)\n",
      "After:  the proposed methods, including kNN, Naive Bayes, logistic regression, and SVM classifiers, are feasible and well-suited for continuous features typical of the dataset. the authors propose k-nearest neighbor classification, Naive Bayes, logistic\n",
      "Before:  The methods do appear to be somewhat properly described. More details could be used to describe each method. i think that the justification for the methods are well described, but how they fit into the actual dataset could be described more (specifics on how you plan to implement models on dataset, esp LR or SVM) The methods are clearly described, detailing the reasoning behind each classifier's choice and the implementation plan for active learning, though additional specifics on model parameterization could enhance clarity Each method is described and a justification is given as to why to use it The purpose of using the methods are described and which can be compared. While the methods are there, as well as the plans for these methods, the exact procedure these methods take is not mentioned. They are properly described on what part of the dataset and why they would want to use the method for the project The authors explain the limitations of logistic regression/SVM to Naive Bayes and the need to select different metrics to examine each classifier's performance. However, they could have gone into further detail on how they intend to use each method (eg: why they are using kNN vs Naive Bayes, etc) yeah it's clear enough 4 models explained plus a discussion of active learning Lack of detailed information on how each method will be specifically tailored or optimized for this dataset, including parameter selection and any modifications to address the dataset's unique characteristics. The specific method selection of each method is explained in detail, and the strategies of cross-validation and active learning are elaborated.\n",
      "After:  the methods are clearly described, detailing the reasoning behind each classifier's choice and the implementation plan for active learning. the authors explain the limitations of logistic regression/SVM to Naive Bayes and the need to select different metrics to examine each classifier's performance.\n",
      "Before:  The methods for the problem do appear to be relevant. All methods chosen appear to be active learning methods, and they are doing a cross comparison of these methods on the dataset. i think that the methods do make sense considering you mention that the dataset is mostly continuous, mention of linear decision boundary classifiers vs tree based classifiers makes sense, could still implement DT or random forest as a comparison though The selection of methods is highly appropriate, leveraging a range of classifiers to address the dataset's balanced nature and continuous features. The incorporation of active learning to optimize sample labeling further aligns with the project's aims kNN - provides a non-parametric model to evaluate, NB - provides a probabilistic approach, LR, SVM - provide approach to linearly separate data kNN, logistic regression, Naive Bayes, and SVM are all good for classification, and it will be interesting to compare their performances. The methods seem to be appropriate. They take from previous literature, so there is backing for them to be used in this project. I think the probabilistic methods would be more useful than linear classifers that they want ot try. The proposed methods are appropriate for the problem of image analysis; kNN is straightforward and evaluates data point similarity to determine regions of malignance, Naive Bayes can deal with the large amount of features present in the images, logistic regression can efficiently be trained to perform the binary classification between healthy and malignant individuals, and SVM is a valid method for capturing complex patterns that the other methods could not. NB SVM and LD are all classifiers, I think it's fair to do so 1. kNN may be too simple for such complexed feature input. 2. Naive Bayes seems okay but potentially hard to train. 3. Logistic Regression may serve as a better model with some linearity. 4. SVM with some kernel trick may give the best performance. kNN Classifier: Well-suited for medical datasets due to its ability to classify based on the similarity of instances, which is key in identifying patterns in breast cancer cells. Naive Bayes Classifier: Appropriate for datasets with independent features, like observable cell traits, offering a straightforward probabilistic approach to cancer diagnosis. Logistic Regression: Ideal for binary classification problems with continuous features, directly linking cell characteristics to the likelihood of being malignant or benign. SVM Classifier: Excellent for distinguishing complex patterns in high-dimensional spaces, making it a robust choice for binary classification in breast cancer diagnosis. NB, KNN, LR, and SVM are all machine learning methods suitable for binary classification problems.\n",
      "After:  kNN, logistic regression, Naive Bayes, and SVM are all good for classification. kNN is straightforward and evaluates data point similarity to determine regions of malignance. logistic regression can efficiently be trained to perform binary classification.\n",
      "Before:  Their evaluation metric appears to be relevant for a binary classifier which is what they are doing for the dataset. evaluation metrics make sense for binary classification : The proposal plans to use a comprehensive set of evaluation metrics, including accuracy, precision, recall, and F1 Score, which are well-chosen for a binary classification problem of high medical significance They clearly describe how they plan to analyze the output of each model They are using accuracy, precision, recall, F1 score, and cross validation to evaluate the methods. Good discussion of evaluations, including cross-validation of models. They describe the analysis properly for the dataset and how they would get proper answers from data. The authors go into detail for each method on how they will determine performance, whether it is to evaluate accuracy/precision/F1 scores, or performing cross-validation and training/testing rounds and examining the simulation error values. class just use accuracy, but seems not mentioned very clear appropriate train-test split, cross-validation, and many other metrics are good for evaluation. The evaluation metrics, including accuracy, precision, recall, F1 Score, and cross-validation, are comprehensive and suitable for assessing the efficacy of machine learning models in this context. It is measured from multiple aspects such as precision, recall, F1 and so on.\n",
      "After:  the proposal plans to use a comprehensive set of evaluation metrics, including accuracy, precision, recall, and F1 Score. they are using accuracy, precision, recall, F1 score, and cross validation to evaluate the methods.\n",
      "Before:  Their related works are directly related to the main problem. Including another ML paper on similar datasets and on domain knowledge papers. related works are good and seem to describe the overall issue well along with efficacy of existing approaches Citing studies that highlight the success of various classification models with this dataset underscores the project's basis in well-established research, supporting the relevance and potential impact of the proposed work​ They clearly describe how the cited papers are relevant and useful to their project Each related work discusses machine learning methods appropriate for classification. The related works are highly relevant to the main problem as they form the inspiration for the motivation behind this project. The articles are pretty relevant in terms of the type of methods they wanted to use for a successful classification outcome. The authors clearly identify the comparative studies and existing classification models for breast cancer diagnosis within their background rationale. These papers examine the importance of examining features such as curvature/continuity within breast tissue imaging, and are highly relevant to the main problem of accurately classifying images to cancer diagnosis. it's basically related to background of the problem Introduction of dataset, historically better models, and the standard way of evaluation are helpful knowledge to understand this field. The selected works are relevant, offering insights into breast cancer prediction using machine learning and supporting the need for comparative studies of classifiers. One of the cited articles uses a similar pipeline and applies the same method, which can provide quite valuable reference data.\n",
      "After:  related works are directly related to the main problem. cited papers highlight the success of various classification models with this dataset. one of the cited articles uses a similar pipeline and applies the same method.\n",
      "Before:  The strengths of the proposal is that the problem seems well defined, and all of the methods appear to be feasible in the time given. well-defined problem with supported approaches in literature, very good description of model evaluation steps The proposal effectively leverages a well-chosen, highly relevant dataset to address a critical medical diagnosis challenge, utilizing a comprehensive array of machine learning methods tailored to the data's characteristics. It demonstrates a methodical approach to model evaluation and selection, grounded in active learning to enhance sample efficiency. The use of a diverse set of evaluation metrics and the inclusion of related work provide a solid foundation and clear direction for achieving the project's goals​ Overall, this proposal is very thorough and provides a detailed description of the methods that will be implemented, the justification for them, and how they will be evaluated The proposal is very detailed about analyzing the efficacy of the models. Overall, this proposal is very well-thought out, based on a good amount of literature that did these methods before, and also seems to be quite feasible. Very detailed in all explanations of methods and background information. it is formatted as well. The authors present a well-written proposal overall that clearly defines the rationale behind selecting their problem and the details behind how they intend to implement and compare their machine learning models. It should be straightforward for them to begin data preprocessing and analysis given the framework they laid out. I think it's good, clearly depict the problem and methods to solve it. Their proposal is a solid piece that focuses on formulating the problem and methods to study it. The proposal targets a clear and important problem, uses an appropriate dataset, and selects suitable methods and evaluation metrics, demonstrating a well-thought-out approach to improving breast cancer diagnosis. The introduction to the method part is very detailed, and cross-validation and active learning are also considered to enhance the universality of the model.\n",
      "After:  proposal leverages a well-chosen, highly relevant dataset to address a critical medical diagnosis challenge. it demonstrates a methodical approach to model evaluation and selection. the proposal is very detailed about analyzing the efficacy of the models.\n",
      "Before:  A weakness of the proposal is that there is a possibility that comparing the different types of classifiers doesn't really lend to the overall research problem of identifying breast cancer as malignant or non-malignant. At least, it appears to be evaluating ML models on the dataset, rather than using the ML models to elucidate new information from the dataset. could use further explanation of how specific models will be implemented, also further description of dataset could be helpful (if large feature space, could use some feature selection method) The proposal could improve by providing more detailed information on the computational demands and the practical implications of deploying the chosen models in real-world diagnostic settings. It may also benefit from a deeper exploration of model interpretability and the potential biases inherent in machine learning-based diagnostic tools. Further, a more explicit strategy for handling imbalanced data, if present, and mitigating overfitting, especially in the context of active learning, would strengthen the proposal​ Missing project title, otherwise great job I think the proposal could benefit from including more information about the dataset and its structure. The main flaw I see is that the proposal does not provide justification for the methods chosen beyond simply being used in primary literature; while this is a good reason, I would have liked to see a summary on why these methods were chosen versus a decision tree or others. Should be condensed to one page. It is a little too dense for what was asked for the proposal. The authors could have provided further detail into their selection of relevant supporting works, such as the main conclusions drawn from the paper and how they might help them in solving their problem. Additionally, it would have been nice to very briefly explain why they are choosing each of the machine learning methods they have chosen, and what each method would tell them about the data. I think for all method use is classify method, maybe can consider add more diverse method like PCA. However, some detailed tricks in each model's actual implementation were poorly discussed. While the proposal outlines a solid plan, it could benefit from more detailed method descriptions and a deeper discussion on the implementation and specific challenges of active learning within this context. The practical significance of this research can be briefly explained.\n",
      "After:  the proposal could improve by providing more detailed information on the computational demands and the practical implications of deploying the chosen models in real-world diagnostic settings. main flaw is that the proposal does not provide justification for the methods chosen beyond simply being used in primary literature.\n",
      "Before:  The problem is clearly stated as cell type classification. While the motivation and the previous literature backing the problem was well-formulated, I was unsure about what exactly the classifier would do (i.e. the different groups into which it would classify - is it just human and mouse pancreas?). It clearly states the problem they are addressing: This study will evaluate 3 automated cellphenotypes assigning models for the human and mouse pancreatic cells data, paving the way for the completion of the future robust analytical framework. The authors clearly identify the problem as trying to find a method of efficient cell type classification given single-cell RNA sequencing data, and seek to analyze cellular heterogeneity amongst human and mouse pancreatic cells. This problem is supported by their rationale of the downsides of manual clustering, and the importance of accurate cell type classification. it's clear, I can find they want to do cell type classification easily I know they want to do cell type classification, but different cell types lack clear explanations. The problem of time-consuming manual clustering in single-cell RNA sequencing data for cell type identification is clearly formulated. In the introduction section, the goal is briefly and clearly introduced: the classification of cell types and their application value. The reasoning and goal is clearly communicated. The question was stated in the introduction section which is to evaluate the ml methods that could automate the process of cell type classification. This is supported with background introduction and summarized from the previous works. The question was stated in the introduction section which is to evaluate the ml methods that could automate the process of cell type classification. This is supported with background introduction and summarized from the previous works. The problem is clearly formulated in terms of its introduction part and its title, from which I got their main idea at the first glance\n",
      "After:  this study will evaluate 3 automated cellphenotypes assigning models for the human and mouse pancreatic cells data. the authors clearly identify the problem as trying to find a method of efficient cell type classification given single-cell RNA sequencing data.\n",
      "Before:  This is a good dataset, with both human and mouse transcriptome data. The data is from previous literature and should be of good quality. It has enough data to divide into training and testing sets for model. The dataset chosen of single-cell RNA sequencing data from human and mouse pancreatic cells is clearly stated, and is appropriate for the task of cell type classification due to the vast amount of cell type markers present in mammalian pancreas. It would have been nice if the authors provided a bit of detail into how the scRNA-seq data looks like (eg: how many samples, how many features, do they look at TPM or FPKM, etc.) GSE14833 dataset is in line with the research target their dataset has broad cell types and many more identifier. But I wonder whether number of cell types is way too many? highly relevant for evaluating automated cell phenotyping methods in human and mouse pancreatic cells. A public and relatively authoritative data set was selected to facilitate horizontal comparison. Chose a well-studied dataset and even across species (may be relevant since there are many mouse models in biology). The dataset was large enough with complexities and relevance. The dataset was large enough with complexities and relevance. This dataset is a relatively clean and well-developed that several previous researches are based on it.\n",
      "After:  this is a good dataset, with both human and mouse transcriptome data. it has enough data to divide into training and testing sets for model. but i wonder whether number of cell types is way too many?\n",
      "Before:  These methods seem feasible to implement. We have done clustering previously in numpy, and PCA should not be considerably difficult. Additionally, SVMs can be done with previously implemented packages. It seems feasible as they can cluster and categorize the data with clustering and PCA The methods proposed (clustering, PCA, SVM) are all feasible given the numerical input data from scRNA-seq, and all serve to identify cell types similar to one another. These methods should all work fine for the numerical sequencing data provided. it's straightforward, just do the three method mentioned(cluster,PCA,SVM) basic clustering and pca dimensionality reduction is the standard way to do it. But the amount of SVM models needed for manny classes maybe computationally expensive and somewhat inefficient. Traditional clustering, PCA/t-SNE for dimensionality reduction, and SVM for classification are established popular methods. Previous tests of the methods mentioned in the project can be found in the references. We have learned these skills in class and they have some previous studies to follow. Each methods was well-established and widely used, and proved to be effective in scRNA-seq classification as suggesting in the related citations. Each methods was well-established and widely used, and proved to be effective in scRNA-seq classification. The methods they want to adopt really reflect we we have learned in class, and I find it make a lot of sense to me.\n",
      "After:  the methods proposed (clustering, PCA, SVM) are all feasible given the numerical input data from scRNA-seq. traditional clustering, PCA/t-SNE for dimensionality reduction, and SVM for classification are established popular methods\n",
      "Before:  The clustering is just described as traditional clustering, it isn't clear what method will be used. There is a little bit of description as to how the methods would be used in this context, but little beyond that - a big problem is that no exact clustering method was given. The methods were not described regarding their procedures. Finally, I was unsure of why clustering and SVM were both used in this case, and why both are needed. Most of the methods are described well and why they want to use that method for what they want to address in. the project. The authors could have provided further detail on what they intend to use each method for; for instance describing why they chose clustering (it could identify clusters with similar cell-types) and how they intend to implement it (kMeans, hierarchical, how many iterations, etc.) it's listed sepeartely, pretty clear they provide a basic list of methods, but lack some details of how they are going to crack the problem. The methods are outlined with sufficient clarity, but details on specific clustering algorithms or the integration of PCA and t-SNE are missing. The method is named but the specific operation is not mentioned. For example, clustering is not detailed to the specific clustering type and standard. It's unclear what type of clustering they will do. For cluster, it needs to specify which clustering algorithms will be explored and the reason of choosing them. For PCA/t-SNE, it is better to illustrate the preprocessing and postprocessing, how to decide the principle component. For SVM, it is better to outline the approach to select the features, detailed process. Besides, for reduced dimensionality, the method did not provided a clear relationship with the reduced and original dimensionality. For cluster, it needs to specify which clustering algorithms will be explored and the reason of choosing them. For PCA/t-SNE, it is better to illustrate the preprocessing and postprocessing, how to decide the principle component. For SVM, it is better to outline the approach to select the features, detailed process. Besides, for reduced dimensionality, the method did not provided a clear relationship with the reduced and original dimensionality. I think all look good, but the description of Method 1 Cluster sounds vague to me.\n",
      "After:  the methods are outlined with sufficient clarity, but details on specific clustering algorithms or the integration of PCA and t-SNE are missing. the authors could have provided further detail on what they intend to use each method for.\n",
      "Before:  The clustering methods (clustering, PCA, and SVM) for classification seem appropriate to solving the problem. Given that SVM takes from previous literature, I believe it to be appropriate; PCA is also appropriate in that it helps to reduce the often high-dimension RNA-seq data. However, I am unsure of where clustering fits into this scenario, especially given that SVM is also used. Clustering: is good for categories of unsupervised cell-type identification, though they did not say which type of clustering they plan to do. PCA: Good to reduce dimensionality and capuring variance within data to selection appropriate features. SVM: good for identify hyperplane that distinctly separates the cell-type in high dimensionality data. Clustering is very appropriate for the question of identifying cell types from sequencing data. PCA for the purposes of feature elimination is appropriate and could be useful in identifying the genes for which expression most greatly dictates a cell type. SVM is also appropriate to identify cell types in a more complex manner, and the idea of identifying a hyperplane to distinctly separate cell-types in high-dimensional space. method is in line with research target to classify cell type 1. what are traditional clustering methods? 2. PCA is the standard way to deal with such data. 3. SVM is good, but may be computationally too expensive for too many classes. Clustering: Essential for unsupervised cell-type identification, leveraging the natural groupings in the data to infer cell types without predefined labels, addressing the need for automated analysis in scRNA-seq data. PCA/t-SNE: These dimensionality reduction techniques are crucial for managing the high-dimensional nature of scRNA-seq data, aiding in visualization and potentially improving the performance of subsequent classification steps by highlighting underlying structures and similarities between cells. SVM: A powerful supervised learning model known for its effectiveness in high-dimensional spaces, making it highly suitable for classifying cells into specific types based on their gene expression profiles, thus providing a precise automated alternative to manual classification methods. Previous tests of the methods mentioned in the project can be found in the references. Clustering - appropriate and common technique for data type. PCA/t-SNE - also very common and useful in single cell data. SVM - appropriate biological application of technique learned in class (distinguishing categories, i.e. cell types). 1. Clustering: this is highly appropriate as it is the fundamental of unsupervised classification when exploring data without prior knowledge. 2. PCA/t-SNE: appropriate because it is crucial for simplifying the complex, high-dimensional scRNA-seq data, however, linear/nonlinear relationship might be missed since this did not mention in the passage. 3. SVM: appropriate because it is capable of handling linear and non-linear boundaries between classes and able to do the classification based on the gene expression. 1. Clustering: this is highly appropriate as it is the fundamental of unsupervised classification when exploring data without prior knowledge. 2. PCA/t-SNE: appropriate because it is crucial for simplifying the complex, high-dimensional scRNA-seq data, however, linear/nonlinear relationship might be missed since this did not mention in the passage. 3. SVM: appropriate because it is capable of handling linear and non-linear boundaries between classes and able to do the classification based on the gene expression. I think they all make sense to me. I would say Method 1 and 3 are reasonable.\n",
      "After:  clustering, PCA, and SVM seem appropriate to solving the problem. however, i am unsure of where clustering fits into this scenario. clustering is good for categories of unsupervised cell-type identification.\n",
      "Before:  All evaluation methods seem appropriate and the ones chosen are tailored specifically to evaluating the method. Fairly simple metrics that are common for these types of models - they could also consider metrics that would fit with the specific scenario. They are using cross-validation to evaluate their data using f1-score and he Jaccard similarity coefficient and Pearson correlation coefficient for unsupervised learning. They also are using some visual classification to validate their model as well. The authors state they will use cross-validation and accuracy/F1-scores to evaluate the performance of the clustering and dimensionality reduction methods, both of which are good metrics of method performance. They also suggest using the ROC curve to visualize the method performance, though it would be interesting to state how they intend to do this (use R/python packages, etc.). seems not mentioned how to evaluate methods very clear maybe can add visualization to data after dimensionality reduction for evaluation The proposed evaluation metrics, including Jaccard and Pearson coefficients for unsupervised methods and accuracy, F1-score, and AUC-ROC for supervised methods, are well-suited to assess model performance. The accuracy and F1-score, Jaccard similarity coefficient and Pearson correlation, ROC curve and other parameters are used to measure the results, and cross-validation is applied to enhance versatility. They chose an appropriate validation for each method (e.g. for unsupervised, Jaccard and pearson corr, while for supervised, accuracy and F1). One important part missing is to evaluate the time complexity and efficiency with comparison of the previous methods. Besides, what are some metrics that could help explain the quality of dimensionality reduction? Other metrics are suitable for the projects as these are standard and common choices. One important part missing is to evaluate the time complexity and efficiency with comparison of the previous methods. Besides, what are some metrics that could help explain the quality of dimensionality reduction? Other metrics are suitable for the projects as these are standard and common choices. The metrics make sense and reflect what we have learned from the class.\n",
      "After:  authors are using cross-validation to evaluate their data using f1-score and he Jaccard similarity coefficient and Pearson correlation coefficient for unsupervised learning. they also are using some visual classification to validate their model as well. seems not mentioned how to evaluate methods very clear\n",
      "Before:  All mentioned works are relevant to the problem in either background or machine learning techniques used for classifying this type of data. The related works discuss the motivation behind the problem as well as previous work that is done on it relating to machine learning and so are quite appropriate. The related work addresses scRNA-seq analysis which is what they project is attempting to tackle for the project. The authors identify six different papers regarding single-cell analysis and classification using RNA sequencing data, all of which are very relevant to the problem they are trying to solve. In particular, the authors indicate that they intend to consult a study into the evaluation of ML approaches for cell-type identification, which could be an interesting look into which classification method performs better given the dataset. all works including Huang 2021 in line with research background well they reviewed many previous work regarding many different model performance and provide a good cohesive understanding of this field. The choice of related works is highly relevant, offering a solid foundation for the proposed methods and highlighting the project's significance in the context of single-cell RNA sequencing. Seven references are cited, covering past research results of various testing methods (cluster, SVM). They site papers that address similar problems and have tried methods such as theirs. The articles are effectively support the questions and method chosen in the proposal and the design of the projects are based on the articles. The articles are effectively support the questions and method chosen in the proposal and the design of the projects are based on the articles. The majority of them make sense, but some of them seem seem not that appropriate for me.\n",
      "After:  related works discuss the motivation behind the problem as well as previous work that is done on it relating to machine learning and so are quite appropriate. the authors identify six different papers regarding single-cell analysis and classification using RNA sequencing data, all of which are very relevant to the problem they\n",
      "Before:  Each section is structured very well and clearly conveys their goals. The proposal has a good motivation for its problem and takes a lot from literature. It uses a dataset that has also been used in previous studies, so it should be a good dataset, and a feasible plan does seem to be in place. Overall, very concise and gets straight to the point of what they want to address. The authors provide a realistic and feasible model plan that involves clustering, PCA, and SVM to identify the cell-type of a sample, and clearly discuss the importance of accurate classification and how their proposed methods can be used with the data provided (eg: benchmarking ML tools, providing an automatic classification pathway, etc.). it's pretty well The proposal is overall solid and a easy to follow framework. The proposal clearly defines a relevant problem, selects an appropriate dataset, proposes feasible methods with a solid evaluation plan, and is well-supported by relevant literature. The content of each section is very detailed and clear. The goal is clear and the 3 methods are well defined. The reasoning is also established for each. 1. The question was clearly described with evident articles to support the validation of the chosen dataset and methods. I really like the introduction part. 1. The question was clearly described with evident articles to support the validation of the chosen dataset and methods. I really like the introduction part. It is well-structured and has a really meaningful topic. The question has been clearly described.\n",
      "After:  each section is structured very well and clearly conveys their goals. the authors provide a realistic and feasible model plan. overall, very concise and gets straight to the point of what they want to address.\n",
      "Before:  The methods need some more detail and justification, but overall good proposal. I am unsure about the exact classifications that are being assigned. Additionally, I believe that the methods could use significantly more thought, especially as to what kind of clustering is performed and why it is necessary alongside the supervised SVM. The one method was not clearly stated. The authors could have provided further detail into their dataset (number of features, number of samples, etc.) as well as further discuss the methods they intend to use by providing a brief summary of the method process (eg: hierarchical clustering) and why they decided to use that particular method over other methods (eg: kNN). maybe add some metric to methods May lack some detailed tricks and actual implementation regarding how to build their models. The proposal could benefit from more detailed descriptions of the specific clustering techniques to be used and how PCA and t-SNE will be integrated for dimensionality reduction. As a proposal, it is longer than other groups. It's less clear what their study will be contributing beyond what the study they cited has already done. 1. Descriptions of method and evaluation are too general with a lack of important details. \n",
      "2. Would the methods be specific to the human pancreas cells or other cells as well? 1. Descriptions of method and evaluation are too general with a lack of important details. \n",
      "2. Would the methods be specific to the human pancreas cells or other cells as well? The methods proposed lack innovation.\n",
      "After:  i am unsure about the exact classifications that are being assigned. i believe that the methods could use significantly more thought. descriptions of method and evaluation are too general with a lack of important details.\n",
      "Before:  it's clear and I know easily they want to classify cancer type they want to train classifiers to subtype the cancer type. The problem of improving cancer subtype classification through machine learning to enhance targeted therapy is clearly and concisely stated. It introduces in detail the causes and difficulties of the research problem, and why machine learning methods should be used for research. The motivation behind their goals are clear. The question was clearly formulated in the first paragraph, which is to overcome the current difficulties of cancer type classification with an optimized solution through machine learning. The introduction part concisely clarifies the problem. Excellent! The statement is short but brief, with easy-understanding sentences. The objective is clear and nice presented. Clearly claim the problem about the classification of cancer subtypes.\n",
      "After:  the problem of improving cancer subtype classification through machine learning is clearly and concisely stated. the statement is short but brief, with easy-understanding sentences.\n",
      "Before:  not mention very clear which dataset what to use I feel like 284 cases and 712 samples seem to be not sufficient for machine learning algorithm to construct a capable classifier. Mention using RNAseq data for various cancers but not specify a particular dataset References are cited that apply to the dataset, but no more specific description of the dataset is given. Existing studies have shown desirable results with this data. The dataset is public and large enough with proper complexity (subtypes) The dataset is well-developed and relative clean that seems been widely used in many research. They don't give the resources of dataset Specific dataset details are not provided. There is no apparent paragraph mentioning the selected dataset.\n",
      "After:  284 cases and 712 samples seem to be not sufficient for machine learning algorithm to construct a capable classifier. existing studies have shown desirable results with this data. the dataset is well-developed and relative clean that seems been widely used.\n",
      "Before:  it's all straightforward. PCA, cluster and classify kNN, pca, mlp shall do fine on their tiny data set. In fact, pca may not even be necessary for this little scale. The methods proposed, including K-means clustering, PCA for dimensionality reduction, and kNN and MLP classifiers, are well-established in the field and feasible for the task. Multi-dimensional RNAseq data is suitable for cluster, PCA, and simple MLP, and the references also confirm this. These are all appropriate methods learned in class, aside from MLP. These are feasible methods are they are widely used and cover the nonlinear and linear relationships. However, the choice of k in knn and k-means might result in different classifications. Overall sounds reasonable, but methods like PCA do not fully convince me of its feasibility. Based on the paper referred, the methods are not difficult to apply The 4 methods are properly proposed, and also revelant to the problem itself. The methods part depicts the ML algorithms will be used and the corresponding metrics.\n",
      "After:  multi-dimensional RNAseq data is suitable for cluster, PCA, and simple MLP. the choice of k in k-means might result in different classifications. overall sounds reasonable, but methods like PCA do not fully convince me of its feasibility\n",
      "Before:  they are pretty clear. they pack great detail, even including plotting, in this proposal about how are they going to do with these models. Some details on the specific implementation could be further elaborated. A specific pipeline description is given, how to use the results of PCA, and how to conduct mutual verification. Since we didn't learn MLP in class, perhaps they could have provided more explanation on it and why it would be useful. The current description is good but would be better with more clarification. For example, it would be better to explain the reason behind the choice of k in k-means cluster. Besides, it could mentioned that how PCA could be further used to help improved better clustering and how to choose the principle component. For the knn and MLP, it would be better if it clarifies how to choose the k/distance matrix and the specific implement of the mlp ( layers, functions for example) would be better. I will stick to my previous argument. Overall sounds reasonable, but methods like PCA do not fully convince me of its feasibility. There are explanations about how to apply the methods specifically The methods are adequately described. Mechanism and function are briefly explained for each method.\n",
      "After:  the current description is good but would be better with more clarification. for the knn and MLP, it would be better if it clarifies how to choose the k/distance matrix. overall sounds reasonable, but methods like PCA do not fully convince\n",
      "Before:  PCA, clustering and classify all match cell type classify well 1. k-means may not have the best performance. 2. pca can reduce dimensionality to their tiny dataset for better visualizations. 3. kNN and mlp may have better performance in terms of classification accuracies. The proposed methods are highly appropriate, utilizing clustering to identify potential subtypes, PCA to visualize separability, and classifiers to accurately subtype cancers. : The methods chosen are appropriate for the cancer subtyping problem for handling multi-dimension data with cluster, PCA and knn(MLP) Clustering - common for scRNAseq and aptly uses hockey stick method. PCA - also common for RNAseq and helpful for reducing noisy and redundant data like RNAseq. Classification - allows them to accomplish the goal of identifying cancer subgroups. 1. Kmean: appropriate because it is a robust method for identification of data with high dimensionality. 2. PCA: it is appropriate because this is the most common tool of dimensionality reduction with retaining variance. But it is worth to notice that if there is nonlinear relationship between the data from dataset. 3. KNN with MLP: Both are appropriate as they could handle such a dataset with different heterogeneity. And this could cover both linear (with knn) and non-linear complex relations (with mlp). MLP would use its hidden layers to learn the relevant features from raw data, making it suitable for this project without much information. Overall sounds great, but methods like classification lacks details (especially for MLP). Using unsupervised clustering in identifying subtypes is appropriate and not complicated The complexity of the methods may not solve, but the methods are releva=nt and useful. Three methods are all appropriate. They are aligned with the order of the workflow. Firstly, cluster the dataset, then perform dimensional reduction and classify the results with the befitting method.\n",
      "After:  the proposed methods are highly appropriate, utilizing clustering to identify potential subtypes, PCA to visualize separability, and classifiers to accurately subtype cancers. the complexity of the problem may not solve, but the methods are releva=nt and useful\n",
      "Before:  it listed out and pretty clear cross-validation, accuracy, precision, recall, and f1-score are important here for evaluations. The selection of metrics including accuracy, precision, recall, f1-score, and the use of cross-validation is comprehensive and suitable for assessing the performance of the models. Uses a combination of accuracy, precision, recall, and f1-score, while applying cross-validation to enhance generality While the metrics are appropriate, I don't quite understand what scikit-learn's \"equivalent packages\" would be. Will they compare to other scikit-learn models? Or will they use the same as their methods, and if so, why would it be different if implemented correctly? Perhaps a validated data set, such as from another experimental setup or comparing or findings from focal paper would be more appropriate. The presented metrics are good as they are commonly used in previous studies. Additional validation and metrics for clustering could be used if needed. Sounds reasonable, will be better if more discussion on their motivation can be applied. Good application of cross validation evaluation can be more specified using e.g., cancer subtyping/potential data imbalances For different types of methods, befitting and various metrics are selected to examine the performance.\n",
      "After:  cross-validation, accuracy, precision, recall, and f1-score are important here for evaluations. i don't quite understand what scikit-learn's \"equivalent packages\" would be. perhaps a validated data set\n",
      "Before:  Both Au's work and Ferro's work match well with their research target This is where their dataset and model inspirations come from The chosen related works are highly relevant, providing a solid foundation for the proposed project by demonstrating the potential of machine learning in cancer subtyping. The discussion of related works is relevant and provides a good background and could be used to check the result. The works are directly related to their research and provide a good guide for their method. They are appropriate as the project is designed based on the related works. And it also introduced the background of this topic. The majority of them are relevant, but some sound not that relevant. Very related works focusing on the specific topic of identifying the cancer subtypes The relevant works may not fully encompass the current research landscape in cancer subtyping. The related works are both about the unsupervised ML methods and there is no information of the previous work about supervised ML methods.\n",
      "After:  both au's work and ferro's work match well with their research target. the works are directly related to their research and provide a good guide for their method. the relevant works may not fully encompass the current research landscape.\n",
      "Before:  it's clear, and list metric very well their overall path and todo lists are very clear too follow. The proposal clearly articulates a significant problem, proposes a logical sequence of well-established methods for solving it, and selects appropriate evaluation metrics, grounded in a solid review of related works. Clear problem statement, relevant methods, and comprehensive evaluation strategy.Complementarity between methods is illustrated Clear motivation and methods. The proposal was clear with identical questions and backgrounds, with a brief illustration of the method parts. I really like the method description. Concise and well structured. Brief, easy to understand Problem statement/ML methods. The selection of three methods are aligned with the order of the workflow and the description of three methods are detailed and feasible.\n",
      "After:  the proposal clearly articulates a significant problem, proposes a logical sequence of well-established methods for solving it, and selects appropriate evaluation metrics. the selection of three methods are aligned with the order of the workflow and the description of three methods are detailed and feasible\n",
      "Before:  maybe can list out dataset used more clear may provide a more concrete introduction to dataset outside of their \"related work\" part The proposal could be strengthened by specifying a particular dataset for clarity on the appropriateness of the dataset choice and providing more detailed plans for the implementation of each method. Missing introduction to the dataset A bit vague for steps that were not discussed in class, and in how they will evaluate their model. Could be more specific. Can really focus on some details and be more informative! Didn't mention the data source Dataset description/more related works can be better. Didn't specify the dataset and mention the supervised ML methods in the summary of related works part.\n",
      "After:  proposal could be strengthened by specifying a particular dataset. could provide more detailed plans for the implementation of each method. could be more informative.\n",
      "Before:  Well-written and clear problem statement and goals. It is clearly articulated with the significance of seRNA-seq in understanding of the mechanisms of obesity with mentioned machine learning techniques. Really excellent introduction to allow me getting their main idea at the first glance! Short and clear It is a bit general. The problem is complete, including the topic, the problem and the objective. The problem statement clearly identifies the goal of using machine learning techniques to identify genetic loci associated with obesity. However, it could be more explicit about the expected impact or specific applications of these findings. I do think the problem is clearly well formulated, as it tackles a huge public health issue: obesity. I think the \"why\" is answered very clearly. They have problem and objective section to address the problem, and it's clear on goal and objective. The problem statement is clearly formulated, as it identifies obesity as a significant health issue and highlights the use of single-cell RNA sequencing to understand its genetic loci and associated metabolic disorders.\n",
      "After:  the problem statement is clearly well formulated, as it tackles a huge public health issue: obesity. it highlights the use of single-cell RNA sequencing to understand its genetic loci and associated metabolic disorders. it could be more explicit about the expected impact or specific applications of\n",
      "Before:  It's a bit unclear to me what exact dataset they will be using. More details, such as the source material for the study (obese vs. non-obese human, mouse, etc.), would be helpful. It would be better if the dataset would be stated more clearly. I think AMP1 is appropriate because it is an important gene regulating the glucose level as suggested in the related articles, and I think it would be better if a wide range of genes could be used. Well-developed dataset that has received intensive peer reviews (several publications and research base on it). don't give the dataset source No specific dataset is mentioned. Although I can understand the project will study the genes data about the obesity and may use the data mentioned in the related works, there exactly is not related paragraphs describing the dataset will be used. Assuming the dataset involves single-cell RNA-Seq from mice with a focus on obesity-related genetic markers, this seems highly relevant. The score is not perfect because the proposal doesn't detail the dataset's comprehensiveness or diversity. The only reason why I didn't give this a 5 is because I'm not entirely sure how good RNA-seq data would correlate to obesity, if many factors are environmental and not genetic. Of course, these environmental variables can cause genetic changes that can then be reflected in the genetics, so I am very curious to see what conclusions this group will come up with. This also seems quite innovative. They didn't explicitly describe on the dataset, it's not very clear how they choose the datatset. The dataset choice appears appropriate, as single-cell RNA sequencing data can provide detailed insights into gene expression profiles across various cell types relevant to obesity.\n",
      "After:  the proposal doesn't detail the dataset's comprehensiveness or diversity. the only reason why i didn't give this a 5 is because i'm not entirely sure how good RNA-seq data would correlate to obesity, if many factors are environmental\n",
      "Before:  These were techniques learned in class and can be reasonably implemented. The given methods are feasible as they are well-established techniques in these related fields. PCA would reduce the dimensionality and the k-means would help identify the clusters of different cell types to show the heterogeneity. Applying linear regression would review the associations between gene expressions and obesity phenotypes. The methods all sound reasonable to me, but I am afraid that they do not provide sufficient justifications on them. PCA\\Kmeans\\LR is easy to conduct The methods sound great for the \"dataset\". There are detailed description about the function and mechanism for each method. PCA, K-means clustering, and linear regression are standard methods that should be manageable. The methods are highly feasible. However, I would like a bit more explanation into hyperparameter tuning and any types of regularization that can be used to reduce overfitting. They use standard ML methods, it's matching with the problem and objective. The proposed methods are feasible, as they involve well-established techniques such as PCA, k-means clustering, and linear regression, which are commonly used in bioinformatics and machine learning.\n",
      "After:  the proposed methods are feasible as they involve well-established techniques. k-means clustering and linear regression would review the associations between gene expressions and obesity phenotypes. the methods sound great for the \"dataset\"\n",
      "Before:  Each technique is described in detail, including the ones learned in class, but how they will solve for linear regression coefficients is not stated, as well as whether they will use regularization techniques. Overall it is good as the utilization of these techniques was clearly introduced in the paragraph, and it would be better if explaining how PCA is determined, and what independent and dependent variables are in linear regression model All are good, will be better if they can include more motivations and their own thoughts clear description of how they will apply the method Yes, properly described. Three methods are aligned with the order of the workflow. The proposal clarify the input and the output for each method. The methods are described with a reasonable level of detail, especially considering the assumed target audience. I hoped they would provide more justification for which k to select (elbow method, or something else) They described the definition for each method. And it's clear. The three methods are properly described, providing clear explanations of how each technique is applied and the expected outcomes.\n",
      "After:  each technique is described in detail, including the ones learned in class. but how they will solve for linear regression coefficients is not stated. overall it is good as the utilization of these techniques was clearly introduced.\n",
      "Before:  PCA - makes sense to reduce the dimensions since RNAseq is high dim. K-means - also reasonable and commonly used for identifying cell types. Linear regression - perhaps they could provide more justification for why to use this over other models. It could be useful if there is independence between observations and relationship is linear. 1. PCA is highly appropriate because PCA is a powerful tool for dimensionality reduction when dealing with a dataset composed of many features. In this case I believe PCA would help outline the most significant variants. 2. K-means: I think it is ok to use this. It is effective for grouping cells and the selection of k would be important for this process. 3. Linear regression: it would be appropriate if there is no non-linear relationship in this dataset. I think it is general good because it could model the relationship between genetic expression and obesity phenotype, which is crucial in identify the genetic markers. They are all good, but really they do not provide further justification for so that the methods seem not that problem specific. PCA: just for features engineering, K-means: good; LR: good More details on how each method can be much better for the dataset/problem itself. They are appropriate. The proposal will use the dimensional-reducing data from PCA to train the clustering model, and perform the regression prediction based on the results of the clustering. The methods chosen are appropriate for data dimensionality reduction, clustering, and correlation analysis. They are quite appropriate. I especially think seeing the PCA plot would be interesting and trying to identify if there is anything that is clearly separable in the data. It's fairly appropriate. I believe these methods are also some of the methods used in the related papers, thus giving justifications. The proposed methods are appropriate for the problem being solved, as PCA helps identify key patterns in gene expression, k-means clustering can group cells based on similarities, and linear regression can model relationships between gene expression and obesity levels.\n",
      "After:  the proposed methods are appropriate for data dimensionality reduction, clustering, and correlation analysis. k-means clustering can group cells based on similarities, and linear regression can model relationships between gene expression and obesity levels.\n",
      "Before:  They have identified an appropriate standard that they can compare to. The listed metrics are generally good as they are commonly used in these related fields. it would be better if there is a metric to evaluate the clusters and linear regression(mse or r-squared). All make sense to me and their motivations are clearly addressed. a little sample Can be done better on more details: how to measure success. It only mentioned the workflow of evaluation and not included the specified metrics like the accuracy. Moreover, it didn't talk about the cross validation. I am not sure whether the cross validation is necessary, so, I selected 4 for the missing of detailed information about metrics. The evaluation approach seems reasonable, but more details on the specific metrics (accuracy, precision, etc.) and their justification would help assess their suitability fully. I think that this section could've been done a bit better with more specificity. Talking about the elbow method would've been good. They mentioned to use validity and accuracy, and it's not clear how the validity metric works. The proposed evaluation metrics are appropriate, as they involve comparing clustering results with biomarker identification and using a test dataset to validate the linear regression models.\n",
      "After:  the listed metrics are generally good as they are commonly used in these related fields. it would be better if there is a metric to evaluate the clusters and linear regression(mse or r-squared) all make sense to me and their motivations are clearly\n",
      "Before:  The works are directly relevant to their goal of studying obesity, but it's a bit unclear where the single cell RNAseq comes in. These are appropriate as they provided a background introduction for the problem being asked and the dataset being chosen. All are really relevant, but again I do not see that they strongly support their motivations and ideas. Don't have the ML related paper The related works are proper and relevant, and fit the problem. The related works are absolutely relevant to the obesity. The works are directly related to obesity genetics and provide a necessary background, but additional, more recent studies could enhance the context. It's appropriate because it talks about certain genotypes that might be associated with a higher incidence of metabolic disorders and obesity. The related works study the similar problem or the same problem as theirs, thus it's appropriate. The choice of related works is appropriate, as they highlight relevant genetic factors associated with obesity, providing a basis for the proposed research.\n",
      "After:  the works are directly relevant to their goal of studying obesity, but it's a bit unclear where the single cell RNAseq comes in. all are really relevant, but again I do not see that they strongly support their motivations and ideas.\n",
      "Before:  Well-researched background on clinical motivation. Nice summary, overall brief but detailed. I really like the topic. Well-structured, details-included, and logical. Brief, Easy to understand and conduct Relevant works, methodological framework, methods used. Three methods are appropriate. - Direct relevance to a significant health issue.\n",
      "- Combines multiple analytical methods for a comprehensive approach.\n",
      "- Plans for both internal validation (clustering reliability) and external validation (predictive model testing). The objectives are pretty clearly defined. I also think validating with existing biomarkers is a good strategy. Their strength is that they have a clear definition of the problem, objective, and the definitions and the usages for the methods are clear. Overall strengths of the proposal include its clear problem formulation, appropriate dataset choice, feasibility of methods , and relevance of related works, which all contribute to a comprehensive approach for studying obesity using single-cell RNA sequencing.\n",
      "After:  well-structured, details-included, and logical. objectives are pretty clearly defined. plans for both internal validation (clustering reliability) and external validation (predictive model testing)\n",
      "Before:  Some details regarding how linear regression will be carried out and the data to be used are somewhat unclear. Could be more specific if needed. Sometimes fail to make the approach problem-specific enough, which means that their approach sounds like too general (how and why do they think the proposed methods can work well on addressing the problems). Too sample to give some datasets or related works Specificity in the problem statement/ Dataset/ Evaluation. The parts of dataset and evaluation are absent or partial. - Lacks detailed justification for the choice of methods and their specific application to the data.\n",
      "- The evaluation strategy could be more detailed, particularly regarding metric selection and validation processes.\n",
      "- Additional context from recent literature would strengthen the research's foundation and relevance. Weaknesses come mostly from lack of specificity in the evaluation metrics. One weakness would be the lacking details of dataset and the evaluation metrics are relatively unclear and could need more discussions. One potential weakness of the proposal is lack of discussion on potential limitations or challenges associated with the methods, such as issues related to data preprocessing, model selection, or interpretation of results. Additionally, the proposal could benefit from providing more details on how the results will be interpreted and translated into actionable insights for understanding obesity and its genetic factors.\n",
      "After:  details regarding how linear regression will be carried out and the data to be used are somewhat unclear. could be more specific if needed. sometimes fail to make the approach problem-specific enough. lacks detailed justification for the choice of methods and their specific application.\n",
      "Before:  Good writing Can be more specific about why acetylcholinesterase is chosen. Very clearly. The problem statement is clear and concise, focusing on predicting specific bioactivities for enzyme targeting using QSAR models. It is well-articulated, giving the reader a clear understanding of the project's objectives. I think the problem is formulated somewhat well. It seems to look at molecular fingerprints (assuming Morgan) to predict a number of different values related to toxicity They have explicitly written the problem statement and the objective for the problem. The problem statement is clear as it identifies the need to predict specific bioactivities of different molecules for drug discovery using machine learning, citing relevant literature and objectives. The problem statement is very clear, and the methods and objectives used are also very precise. The problem is stated clearly and detailed in the end of background paragraph, and it is stated clearly and with much more details in the methods section. The problem is clearly formulated. The group aims to use machine learning techniques to predict specific bioactivities (like IC50) for enzymes in molecules, aiding in the identification of analogues for lead compounds. The proposal provides a well-defined background and objective, clearly stating the aim to develop QSAR-like pipelines using machine learning techniques for predicting specific bioactivities of molecules. However, it could have been clearer by specifying early on the significance of the IC50 value and how it directly relates to the project's goals, enhancing the immediate understanding of the project's practical implications in drug discovery.\n",
      "After:  the problem statement is clear and concise, focusing on predicting specific bioactivities for enzyme targeting using QSAR models. the group aims to use machine learning techniques to predict specific bioactivities for enzymes in molecules, aiding in the identification of analogues for\n",
      "Before:  don't give the dataset source dataset is good and have potential. Very good. There is detailed information about the dataset. The choice of the 8832 molecule entries from ChEMBL is appropriate, given the project's aim. This database is reputable and widely used in the field, which supports the relevance and reliability of the chosen data set. Morgan fingerprints is quite easy to calculate and does not take much time to do so. Has the group thought of any other types of datasets as well (like ADMET oriented datasets) They have explicitly point out what dataset they are going to use and the dataset seems to be matching with the problem. The choice of the ChEMBL database containing thousands of molecule entries targeting acetylcholinesterase seems appropriate for the problem as it provides relevant property information about biomolecules and their interactions with target enzymes. ChEMBL is a very comprehensive database that can provide a lot of information The dataset is highly relevant to the topic, which is a database containing detailed information on bioactive molecules with drug-like properties. Moreover, the data is a well-known dataset of high quality and reliability with sufficient data volume. The choice of the dataset from ChEMBL, comprising 8832 molecule entries targeting acetylcholinesterase, seems appropriate. ChEMBL is a well-known database for bioactivity data, making it a relevant source for this study. ChEMBL is widely regarded as a reliable and comprehensive source for biochemical data, essential for QSAR modeling. This dataset provides a relevant and robust foundation for applying and evaluating the proposed machine learning models in a meaningful context related to drug discovery.\n",
      "After:  8832 molecule entries targeting acetylcholinesterase seem appropriate for the problem. data is reputable and widely used in the field, which supports the relevance and reliability of the chosen data set. dataset provides relevant property information about biomolecules and their interactions\n",
      "Before:  Usual models are feasible Seems like they need to handling large datasets with complex ML models. The methods are aligned with the order of the workflow. QSAR might be tough, the project is structured with clear steps, which aids its feasibility. These methods seem highly feasible The methods are applicable on the dataset they chose. The proposed methods of using Linear Regression, Random Forest, and K-Nearest Neighbor models are feasible for predicting bioactivities based on molecular structures, as supported by existing literature and the availability of suitable datasets. These are established machine learning methods used for the tasts of classification and regression. These methods are basic and have strong feasibility. The methods are feasible. LR can tell the relationship between molecular features and bioactivities. Random forest can model non-linear relationships and is robust against overfitting. KNN is good at handle non-linear relationship and is useful for classifying molecules into similar activity classes. The proposed methods - Linear Regression, Random Forest, and K-Nearest Neighbor - are feasible for the task. These methods are standard in machine learning and have been successfully applied in QSAR models previously. The methods mentioned in the proposal are suitable for the task at hand, as they are commonly employed for similar prediction tasks. Also the reliability of the dataset makes the project more feasible.\n",
      "After:  the proposed methods of using Linear Regression, Random forest, and K-Nearest Neighbor models are feasible for predicting bioactivities based on molecular structures. these are established machine learning methods used for the tasts of classification and regression\n",
      "Before:  The explain how to use the methods They described the methods in detail. The methods focus on the classification using supervised ML methods. The descriptions of the three methods are detailed, providing insights into how each will be implemented and evaluated. The explanation shows a good understanding of the methodologies, though more technical depth could further enhance clarity. They're adequately described in my opinion. They explain the usage for each method and the reasons to use them. The methods are properly described, outlining the rationale behind each model choice and how they will be applied to predict bioactivities based on chemical structures. These methods are well described and scenario are appropriate This proposal gives clear description in data preprocessing, the desired machine learning methods and the model evaluation methods. The three methods are well-described in the proposal. Linear Regression is proposed for finding correlations between chemical bioactivity and compound fingerprints. Random Forest is chosen for its ability to handle non-linear relationships and avoid overfitting. KNN is planned to group structurally similar molecules. This description provides a clear understanding of how each method will be used. The proposal provides a basic overview of the three machine learning techniques and briefly mentions their application to the project. However, the descriptions lack depth and specificity regarding how each method will be uniquely adapted or applied to the QSAR modeling challenge at hand.\n",
      "After:  the descriptions of the three methods are detailed, providing insights into how each will be implemented and evaluated. the proposal provides a basic overview of the three machine learning techniques. however, the descriptions lack depth and specificity regarding how each will be uniquely adapted or applied to the project at\n",
      "Before:  LR: good, RF: not used and don't know, KNN: good Linear Regression for linear relationships, Random Forest for non-linearities, and KNN for structural similarity among molecules. For each model, the proposal explains the function and the detailed mechanism. Linear Regression: Appropriate for establishing baseline relationships between features and target variables, given the dataset's nature. Random Forest: Suitable for capturing non-linear relationships and providing robustness against overfitting. K-Nearest Neighbor: Relevant for leveraging structural similarities among molecules to predict bioactivity, aligning well with the problem's nature. They seem appropriate, as they're testing a number of different ML algorithms They are pretty appropriate. They are common ways to compute the objective. Linear Regression is suitable for identifying linear relationships between compound fingerprints and bioactivity, Random Forest is good at capturing nonlinear relationships, and K-Nearest Neighbor is suitable for identifying structurally similar molecules and predicting bioactivity based on neighbors' properties. Hence, they all seem appropriate. LN can generate the correlation and be used in downstream RF. KNN is a good method for varification in this problem LR is moderately appropriate since the linear assumption may not always hold, which will potentially limit its effectiveness, so random forest and KNN can well make up for it and both of them are of highly appropriateness. Linear Regression is appropriate for identifying linear correlations but might be limited if the relationships are complex or non-linear. Random Forest is a robust choice for handling complex datasets and can provide more nuanced insights compared to linear models. KNN is suitable for finding patterns based on structural similarity, which is important in QSAR models. Linear Regression is suitable for establishing a baseline model and for cases where the relationship between molecular features and the target variable is linear. Random Forest is particularly well-suited for QSAR modeling because it can capture non-linear relationships and interactions between features without the risk of overfitting inherent to decision trees. This method is robust to noisy data and can handle a high dimensionality of features, such as molecular fingerprints, making it highly appropriate for predicting bioactivities of molecules with complex structural characteristics. KNN is highly relevant for QSAR modeling because it relies on the premise that similar molecules will exhibit similar activities.\n",
      "After:  Linear Regression is suitable for establishing baseline relationships between features and target variables. Random forest is a robust choice for handling complex datasets. K-Nearest Neighbor is suitable for identifying structurally similar molecules.\n",
      "Before:  Intro to cross validation and calculating rootmean squared errors (RMSE) and R-square Only RMSE and R^2, may not fully capture the model's predictive performance. There are various metrics for different methods. Using cross-validation, RMSE, and R-square for evaluating model performance is appropriate,. It will give a comprehensive assessment of prediction accuracy and model generalization. Quite unclear as to what exactly is being evaluated here (IC50, toxicity, and a couple other things as well?). Is this regression or classification? They are all applicable to their problem and their dataset The proposed evaluation metric of using root mean squared error (RMSE) and R-square for model performance evaluation is appropriate for regression models, providing quantitative measures of how good th efit is. This project use root mean squared errors (RMSE) and R-square as evaluation metrics Using cross-validation, RMSE, and R-square for model evaluation is standard and feasible. The proposal mentions the use of cross-validation, RMSE, and R-square for evaluating model performance. These metrics are appropriate for regression problems and will help in assessing the predictive accuracy of the models. The proposal specifies the use of cross-validation, root mean squared errors (RMSE), and R-square (R²) as evaluation metrics, which are standard and highly relevant for assessing the performance of regression models in QSAR studies.\n",
      "After:  Using cross-validation, RMSE, and R-square for model performance evaluation is appropriate,. it will give a comprehensive assessment of prediction accuracy and model generalization. the proposal mentions the use of cross-validation, RMSE, and R-square\n",
      "Before:  The paper do have the same topic with The proposal can be better if clearly identified how they are related to the work itself. They are appropriate. The mentioned works are relevant, showing progress in the field and providing context for the proposed study. However, broader coverage of related literature could enhance this aspect. I think the works were good, but I think the group needs to go more in depth about these works. The related works address the same problem or similar problems. The choice of related works is appropriate as they demonstrate progress in predicting molecule properties using machine learning methods, providing relevant context for the proposed project. These works discussed QSAR workflow, which highly related to this project The related work is highly relevant to the topic, which provide a brief introduction of how QSAR is applied to analyze the relationship between the structural features and its biochemical properties. In addition, the paperwork also mentions some of the aspect that this project will discover into. The related works cited are relevant and current, providing a good background on QSAR models and their applications. The selected related works showcase advancements in predicting biochemical properties through QSAR models and the successful application of machine learning techniques, like random forest, in drug discovery contexts. However, a more diverse range of related works that directly compare the effectiveness of the proposed machine learning models in QSAR modeling could enhance the relevance.\n",
      "After:  related works are relevant, showing progress in the field and providing context for the proposed study. broader coverage of related literature could enhance this aspect. these works discussed how QSAR is applied to analyze the relationship between the structural features and its biochemical properties.\n",
      "Before:  Format, Clarity, Good idea Dataset, idea, and suitable methods used. It is very comprehensive and clearly arranged. The proposal is well-structured with a clear objective, employs robust and varied methodologies, and uses an appropriate and well-regarded dataset. It also demonstrates a good understanding of the evaluation process for the proposed models. Strengths: very interesting problem, and should yield sufficient results that are feasible. The strength is that they have explicitly written what dataset they are using. The proposal demonstrates a clear understanding of the problem, provides a thorough review of related literature, and proposes appropriate methods for tackling the stated problem. The selected methods and dataset are very appropriate The proposal has many strengths, including feasible machine learning methods and its potential functions when being implemented, a well-known dataset, data preprocessing methods and model evaluation approaches. I am looking forward to the completion of this project and its potential significance in advancing our understanding of QSAR model. Appropriate and well-defined dataset choice. A good mix of machine learning techniques suitable for QSAR modeling. The proposal has a well-defined objective, appropriate dataset choice, and comprehensive methodology. All of these make the project sounds very feasible.\n",
      "After:  strength is that they have explicitly written what dataset they are using. proposal has many strengths, including feasible machine learning methods. proposal has a well-defined objective, appropriate dataset choice, and comprehensive methodology.\n",
      "Before:  don't give any dataset sources Evaluation/methods can be in detail introduced, and also feasible problem. There is not a direct linked for the dataset. The proposal could benefit from a deeper technical explanation of the methods and a more extensive review of related literature. QSAR also might be a bit difficult, but please, prove me wrong! Weaknesses: not enough specificity about which evaluation metrics. Not sure if classification or regression. Also unsure what target columns (IC50, toxicity, etc) really are. The weakness would be it seems like their proposal is over 1 page, which might not be a fair comparision with others. While the proposal outlines the methods and evaluation metrics, it lacks discussion on potential challenges, limitations, or considerations in the implementation of the proposed models, which could impact the validity and generalizability of the results. Also, in my opinion, using a dimensionality reduction technique alongwith the given methods can be helpful. This is because molecules would have a high number of features which probably would be correlated. Hence, dimensionality reduction may help here. A little bit long Given random forest and KNN can well discover the non-linear relationships that LR cannot, these two methods are really important for this project. However, they may both create a model that is not easily interpretable. In this way it could limit the ability to reveal the underlying relationships between molecular features and bioactivities. To solve this potential problem, probably it is a good idea to apply model-agnostic interpretability techniques as well. The proposal could benefit from a more detailed discussion on potential challenges, especially related to data quality and model interpretability. There is lack of methodological detail and limited discussion on related work in this proposal.\n",
      "After:  weak points: not enough specificity about which evaluation metrics. not sure if classification or regression. also unsure what target columns (IC50, toxicity, etc) really are. proposal is over 1 page, which might not be a fair comparision with others, says\n",
      "Before:  The problem statement is clearly articulated, establishing the importance of accurate cancer diagnosis and the intent to identify the most effective machine learning technique using the Wisconsin Breast Cancer dataset. This group did a great job explaining the problem and how to solve it. The write up was well written The problem of breast cancer prediction is clearly defined, highlighting the importance of accurate diagnosis and the relevance of machine learning techniques for prediction. This project uses machine learning to solve the problem of breast cancer classification After reading the proposal, it is very clear what is the problem to be solved and the introduction is well-written and clear-stated. The group aims to apply machine learning techniques to the Wisconsin Breast Cancer dataset to distinguish between benign and malignant tumors, identifying the most effective method for this classification. They've articulated the importance of accurate cancer diagnosis and the role of machine learning in this context. The proposal clearly states the problem: applying machine learning techniques to the Wisconsin Breast Cancer dataset to identify the most effective method for predicting whether a tumor is benign or malignant. They even wrote a complete version of background, very in detail. The research question of ML to generate cancer-prediction model is clearly identiofied and with significance explained. The idea is a bit idealized for real application but is a very good question for a sample data analyzing project. the problem is clearly formulated They clearly stated the traditional manner in which diseases are classified, mentioning the limitations of these methods. They then described how they will address this challenge through their methodology.\n",
      "After:  this project uses machine learning to solve the problem of breast cancer prediction. the group aims to apply machine learning techniques to the Wisconsin Breast Cancer dataset to distinguish between benign and malignant tumors. they've articulated the importance of accurate cancer diagnosis and the role of machine learning in\n",
      "Before:  The choice of the Wisconsin Breast Cancer dataset is highly appropriate as it is widely used in similar studies and contains relevant features for tumor classification, making it suitable for the objectives outlined. The Wisconsin Breast Cancer Dataset is a great choice for a dataset. The Wisconsin Breast Cancer dataset is appropriate as it contains relevant tumor information and diagnosis labels necessary for training and testing machine learning models for cancer prediction. The dataset is very informative The dataset contains comprehensive data about the clinical description of tumor and it is suitability for binary classification. The Wisconsin Breast Cancer dataset is a relevant and widely used dataset in cancer diagnosis research. It contains essential features for tumor classification, aligning well with the project's objectives. The Wisconsin Breast Cancer Dataset is highly suitable because it is directly related to the problem at hand, has been studied in similar researches, and includes a binary target variable indicating the tumor's nature. They focused on the specific cancer type the breast cancer and provided the specific dataset and explained the dataset in detail. The data set chosen is very related to the question adressed and has been utilized by previous research as a validation for feasibility. The size of the dataset is also appropriate for our model training. it is appropriate as it is common used in similar research for tumor classification They have cited relevant works that have been trained on that dataset.\n",
      "After:  the Wisconsin Breast Cancer dataset is a relevant and widely used dataset in cancer diagnosis research. it contains essential features for tumor classification, aligning well with the project's objectives. the size of the dataset is also appropriate for our model training.\n",
      "Before:  Seems totally feasible given what we've covered/will cover in class. I do think this project is feasible depending on the ML skill level of the group. However, it doesn't seem feasible if no one in the group has any ML experience. The proposed methods of logistic regression, K-nearest neighbors (KNN), and Support Vector Machines (SVM) seem good for the task of binary classification task like tumor prediction. These methods are very feasible and well combined The feasibility of the proposed methods is high. They are standard and popular methods for analyzing data like this and I think they will provide promising result about tumor classification. The methods chosen are standard and feasible for classification problems. All of the methods are standard, well-understood, and widely applied machine learning techniques for supervised learning tasks like tumor classification, where the goal is to predict a binary outcome based on input features. The proposal also mentions adjustments and optimizations specific to each method. Except for three basic models, they also put forward the improved version of SVM which is quite complex to me. The proposed methods has been verified by reference research that has best performance among alls on the same dataset, and the method chosen of Logistic regression and SVM seems to fit naturally with the classification. One question might be why choosing KNN as the third method. The proposed methods (logistic regression, KNN, and SVM) are highly feasible and standard for classification problems in machine learning The methods can be implemented on the dataset.\n",
      "After:  the proposed methods of logistic regression, K-nearest neighbors (KNN), and support vector machines (SVM) seem good for the task of binary classification task like tumor prediction. the proposed methods can be implemented on the dataset.\n",
      "Before:  The descriptions are detailed, providing rationale and implementation strategy for each method. However, a bit more detail on the specific implementation nuances or parameter choices could enhance clarity. They talk a lot about logistic regression, K-nearest neighbors (KNN), and Support Vector Machines (SVM) and how they work. I think this is good. The methods are adequately described, outlining the reason behind each choice and potential adjustments to handle dataset characteristics like multicollinearity. All three methods are described very correctly and are used appropriately The descriptions of these three methods are reasonable and detailed. LR works good for binary outcomes, benign and malignant, and it is suitable for categorical variables that are used to describe the characteristics of tumor. KNN is good at analyzing dataset and reveal the relationships between the tumor features and tumor classification. The description of SVM appropriately highlights the logistics of choosing kernels and regularization. Logistic regression will be used with feature selection to handle multicollinearity. KNN is chosen with a k-value of 3 to balance computational efficiency and accuracy. SVM is planned with Linear and RBF kernels, with a preference for Linear due to the dataset characteristics. The proposal offers a concise explanation of how each of the three machine learning techniques will be applied to the tumor classification problem. While these descriptions provide a solid overview of the intended applications of each method, a more in-depth discussion on the specific configurations (e.g. feature selection criteria) and the rationale behind these choices could further enhance the clarity and completeness of the methodological descriptions. Yes, they have detailed methodologies part. Each method are explain for why they are chosen (based on the research article) and how they output desirable conclusion from by modeling the input data. Also, for each method the specific implementation for how the data are processed and the model are established are detailed demonstrated. logistic regression, K-Nearest Neighbors (KNN), and Support Vector Machines (SVM) What each method will be used for is mentioned appropriately.\n",
      "After:  logistic regression, K-nearest neighbors (KNN), and support vector machines (SVM) are described. a bit more detail on the specific implementation nuances or parameter choices could enhance clarity. logistic regression will be used with feature selection to handle multicollinearity\n",
      "Before:  Logistic Regression: Appropriate due to its efficacy in binary classification, particularly with feature selection to address multicollinearity. KNN: Suitable given the dataset's characteristics and the preliminary finding that k=3 provides high accuracy. SVM: Fitting, especially with the choice of linear kernel to avoid overfitting, and considering the dataset's likely linear separability. They're very appropriate, and I am curious to see which one performs best. Logistic regression, KNN, and SVM are appropriate for the problem. Logistic regression is suitable for binary classification, KNN is effective for well-separated classes, and SVM is good for nonlinear decision boundaries (by using different kernel functions). The proposed methods are appropriate, KNN is used to reduce the complexity and SVM will be smoothly run. LR, KNN and SVM all are appropriate. LR is good at performing binary classification problems, that is to determine whether a tumor is benign or malignant. KNN is good at dealing with the cases where the relationship between features and class labels is complex, and with the cases that local similarity needs to be considered in classifications. SVM is good at dealing with problems with high-dimensional data like Wisconsin Breast Cancer dataset. Logistic Regression is suitable for binary outcomes like tumor classification and can effectively handle categorical data. KNN is appropriate given the dataset's characteristics, with a chosen k-value indicating a balance between accuracy and runtime. SVM is a robust choice, especially with the use of linear and RBF kernels which seem suitable for the linear separability of the data. Logistic regression is ideal for binary classification problems. KNN is particularly useful in cases where the decision boundary is not linear. SVM is powerful for binary classification problems, especially due to its flexibility in handling both linear and non-linear data through the use of different kernels. They will complete the three basic classification method. For SVM and Logistic regression the models identified fits naturally for getting discrete output from the binary input, and their effectiveness are proven with previous works. For KNN it can also be used for classification however maybe it will be better if the author can justify it is the best among all the other choises. The methods are properly described, providing insights into their application and specific adjustments for the dataset Logistic regression and SVM are designed for binary classification. Linear regression can also be used for logistic classification.\n",
      "After:  logistic regression, KNN, and SVM are appropriate for the problem. LR is suitable for performing binary classification problems. KNN is effective for well-separated classes and SVM is good for nonlinear decision boundaries.\n",
      "Before:  The chosen evaluation metrics, including confusion matrix, accuracy, precision, recall, and F1-Score, are comprehensive and suitable for assessing the performance of binary classification models in a medical diagnosis context. I love how in depth they went into this section, talking about log-odds and handling multicollinearity The proposed evaluation metrics, including confusion matrix, accuracy, precision, recall, F1-score, etc. cover a wide range of performance measures commonly used for assessing model performance in binary classification. So they are good. The evaluation metrics are well described including Confusion Matrix, Accuracy, Precision, Recall, Sensitivity/Specificity, F1-Score, AUPRC, Cross-Validation The proposed evaluation metrics, Confusion Matrix, Accuracy, Precision, Recall (Sensitivity), Specificity, F1-Score, Area Under the Precision-Recall Curve (AUPRC), and Cross-Validation, are highly appropriate for tumor classification in the project, since they are all standard and applicable assessing approaches and can directly provide information for clinical uses (for diagnosis, prevention or prognosis). These metrics are appropriate for evaluating classification models and will provide comprehensive insights into the performance of each method. The proposal selects a comprehensive suite of evaluation metrics which are highly relevant and standard for evaluating classification models, especially in medical diagnostics contexts. They lists various evaluation methods. The author proposed many verification used for testing the accruacy of the prediction, with applying cross validation and other numerical evaluators. One interesting thing for SVM and Logistic regression is the possibility for comparing the separation plane of two methods. The chosen evaluation metrics, including confusion matrix, accuracy, precision, recall, and F1-score, are very appropriate for assessing classification models. They mentioned the evaluation metrics that we discussed in class and can compare them to verify their correctness.\n",
      "After:  the proposed evaluation metrics, including confusion matrix, accuracy, precision, recall, and F1-score, cover a wide range of performance measures. these metrics are appropriate for evaluating classification models and will provide comprehensive insights into the performance of each method.\n",
      "Before:  The cited works are highly relevant, discussing similar methodologies and the same dataset, which supports the paper's research context and methodological choices. Went very in depth into this and talked about strengths of each problem and what they did to achieve the results they did. The related works are relevant as they also did similar work using regression and classification techniques on the same dataset, offering comparisons and benchmarks for evaluating the methods. These works focus on the selected dataset and the method adopted, which is very relevant The related paper was discussed regarding this dataset and the methods proposed. One is about the classification of this dataset using machine learning methods, and the other is about using machine learning methods to investigate breast cancer prediction and diagnosis. These two aspects are closely related to the project they want to do, since one is about how to analyze this dataset and the other is about the algorithms of analyzing breast cancer classification. The related works cited are relevant to the project, providing insights into similar studies using the same dataset and techniques. The selected related works directly address the main problem of tumor classification using the Wisconsin Breast Cancer dataset, which aligns perfectly with the proposal's objectives. They choose two paper and explained each about their methods and data, one of them used the same dataset. The research work mentioned did very similar research as the proposed project and analyze the same dataset, so it provides justification for why the models are chosen in the project and provide reference for what data processing procedures can potentially be applied for improving the performance. Two related works are discussed, providing context and comparison for the proposed methods. It describes the author's approach to the main problem. They can then compare their results with the ones in the paper.\n",
      "After:  cited works discuss similar methodologies and the same dataset. related works are relevant as they also did similar work using regression and classification techniques on the same dataset. the selected related works directly address the main problem of tumor classification using the Wisconsin breast cancer dataset.\n",
      "Before:  The proposal is well-structured, addressing a critical healthcare problem with a clear objective. It employs standard, well-justified methodologies and evaluation metrics, demonstrating thorough understanding and appropriate contextualization. Strengths: many; this paper did a really good job at going through every evaluation metric, dataset, and model used. The proposal is well described, stating the problem, literature, and methods clearly. The main strength of it I can think of is that they have a backing of a lot of data and related work previously done. So it would be relatively straightforward to get back on track if the project goes wayward. It is very detailed in terms of methods, data sets and descriptions of related work The topic this project chose is meaningful as breast cancer is a most commonly diagnosed cancer in women. And this project did well in choosing machine learning methods, choosing dataset, implementing evaluation approaches and addressing data quality. I am looking forward to the result of this project to improve the early-stage diagnosis of breast tumor. The project addresses a significant and relevant healthcare problem with a clear objective.\n",
      "A well-chosen and widely recognized dataset. The proposal thoughtfully selects machine learning techniques (logistic regression, KNN, and SVM) that are well-suited to the binary classification task, offering a mix of simplicity and computational efficiency alongside the capability to handle complex patterns. The choice of comprehensive evaluation metrics is another strength. Very detail and clearly explained each part. The work provide very detailed description of the research question and comprehensive introduction for reference work. Also, the proposal provides abundant detail for how the methods are implemented and why they are designed as such. The proposal is well-structured with a clear problem statement, appropriate dataset and methods, thorough method description, and relevant evaluation metrics. The strengths are that all the methods are described properly and a deep literature review has been conducted.\n",
      "After:  the proposal addresses a critical healthcare problem with a clear objective. it employs standard, well-justified methodologies and evaluation metrics. the main strength of it is that they have a backing of a lot of data and related work.\n",
      "Before:  While the methodology is sound, the paper could benefit from a broader literature review to contextualize its approach within current research. Additionally, more detail on specific data preprocessing steps and potential challenges or limitations of the selected methods could strengthen the proposal. I do not think this is unfeasible by any means, but it could be if the group does not have a strong ML background. The proposal was a bit too long. Additionally, it could benefit from more detailed explanations of feature selection techniques and model tuning strategies. Too long Personally speaking, clinical data including the describable statistics of tumor is likely to require data preprocessing, for example, to transfer data like “strong positive, positive, mild positive, negative” to ordered variable like “4, 3, 2, 1”, if this is true for this dataset, it might be better to involve data preprocessing methods. In addition, it might be better to provide information about how the optimal number of neighbors was determined in KNN, for example, metric for distance calculation or the process for selecting the three neighbors. And it might also be better to provide additional details on how the kernels will be evaluated against each other and the criteria for choosing the optimal one. Moreover, ethical considerations regrading clinical data privacy may also need to be mentioned. It may benefit from exploring more advanced or ensemble machine learning techniques that could provide better insights or performance. While the proposal describes the chosen methods and their application to the dataset, it could benefit from a more detailed explanation of the model configuration, parameter selection, and feature selection criteria to enhance reproducibility and understanding. It is kind of too lengthy for just a project proposal. The contents are very good, however, this might be too much for the requirement of the proposal ( the suggested one page) and considering the content it might be too much for the overall scale of this project. The proposal is detailed but somewhat lengthy for a concise review. There aren't any apparent weaknesses.\n",
      "After:  the methodology is sound, but the paper could benefit from a broader literature review. the proposal was a bit too long. it could benefit from more detailed explanation of feature selection techniques and model tuning strategies.\n",
      "Before:  Use RNA-seq data to enhance cancer diagnosis and prognosis The problem is concise and clear. This proposal highlights the significance of the problem and give a concise and clear methods to addressing it. The group aims to leverage machine learning techniques to classify cancer stages using RNA-Seq gene expression data, addressing limitations in traditional classification methods and contributing to personalized treatment strategies. The proposal clearly outlines the goal of utilizing machine learning techniques to classify cancer stages based on gene expression data from RNA-Seq analysis. The problem is simple for understanding and clearly described in both its title and paragraph 1. Clear indication of that they are studying and justify why it is workable in first paragraph. the problem clearly described in paragraph 1 They clearly stated the traditional manner in which cancer is traditionally classified, mentioning the limitations of these methods. They then described how they will address this challenge through their methodology. It is stated as using RNAseq data to predict cancer stages.\n",
      "After:  the group aims to leverage machine learning techniques to classify cancer stages using RNA-seq gene expression data. the proposal highlights the significance of the problem and give a concise and clear methods to addressing it.\n",
      "Before:  only indicate the RNA-seq data The proposal mentioned large-scale dataset TCGA but didn’t give information about how will the methods applied to this dataset (such as which part of the dataset will be used) or why this dataset is suitable for this project. Gene expression dataset obtained by RNA-seq aligns with recent trends in personalized medicine and is suitable for the complexity of cancer stage classification. TCGA is known for its comprehensive collection of cancer genomic datasets, including RNA-Seq data for various types of cancer, making it suitable for the project's goal of cancer stage classification based on gene expression. However, some more detailed elaboration on what specific data to be chosen from TCGA and the reason behind would benefit the proposal. We just know they will extract data from the TCGA platform, but there is lack of detailed information for the chosen dataset and they haven't provide a specific dataset yet. Data type is reasonable. It will be good to indivate where you get the data and for example what type of cancer data you choose, and what are the size and dimension? the dataset from rna-seq is commonly used The dataset could have been explained more clearly. However, based on the description they provided, the dataset appears appropriate. Dataset is RNAseq data. Although no description of what set of transcripts were been analyzed. Also, the transcription pattern would be different for different kinds of cancers (even caner in the same origin may be developed via different pathways). Would like more specific information on those.\n",
      "After:  gene expression dataset obtained by RNA-seq aligns with recent trends in personalized medicine. TCGA is known for its comprehensive collection of cancer genomic datasets. some more detailed elaboration on what specific data to be chosen from TCGA would benefit the proposal\n",
      "Before:  The proposed methods are fessible for RNA-seq data analysis The methods regrading data preprocessing, binary classification and evaluation are feasible. The methods proposed are standard and feasible for binary classification problems. The inclusion of pre-processing techniques like PCA for dimensionality reduction further supports the feasibility of their approach. The proposal outlines a plan to utilize well-established machine learning algorithms for binary classification of cancer stages based on gene expression data. The inclusion of standard preprocessing techniques to normalize and clean the RNA-Seq data further supports the methods' feasibility by addressing potential issues with high-dimensional data. There are strong proof that there are many methods applied and proved success already according to the references. The chosen method should work well for classification. However, specific feasibility will depends on how the data looks like so can not have certain word before we see the specific data chosen by the author. the classification methods are feasible The methodologies they have proposed can be implemented easily without using Python packages. Not enough info on inputs for me to decide. Although based on the output (1-4), the methods seem approprate.\n",
      "After:  the methods proposed are standard and feasible for binary classification problems. the inclusion of standard preprocessing techniques to normalize and clean the RNA-seq data further supports the methods' feasibility.\n",
      "Before:  The usage for the methods are proper The proposal presents feasible data preprocessing approach, PCA, but it is not mentioned how many principal components will be used or how they will be selected. Random Forest, K-NN and Logistic Regression are suitable for this topic and are all standard methods of tumor stages classification. Moreover, the description correctly identifies important metrics for evaluating the performance of classification models, especially in a medical context where both the identification of true positives and the correct classification of all relevant cases are crucial. Random Forest, KNN, and Logistic Regression are standard choices for binary classification and are well-suited for handling the complexities of gene expression data. Though the proposal provides a brief overview of the three methods, the descriptions lack detailed information about the implementation specifics. Additionally, it does not describe how the pre-processing will be integrated into the algorithms. They only mention that which methods will be used but there is no more description. Classification methods are mentioned, but should provide more explanation on how the result of clustering and classification can directly relate to one of the specific cancer stage. the methods are not described in details enough Enough description is provided for each method for the project proposal. No justification for methods choice what so ever. Only names listed.\n",
      "After:  the proposal presents feasible data preprocessing approach, PCA. however, it is not mentioned how many principal components will be used. random forest, K-NN and Logistic Regression are suitable for this topic.\n",
      "Before:  They are appropriate for dimensional analysis and binary classification All the models are highly appropriate. Random Forest is appropriate for its robustness and ability to handle high-dimensional data, which is common in gene expression datasets. KNN could be effective in identifying stages based on similarities in gene expression patterns. Logistic Regression might be useful for its simplicity and interpretability, especially in a binary classification setting. Random Forest is robust against overfitting and capable of handling high-dimensional data, making it suitable for analyzing complex gene expression datasets. While KNN can perform well in identifying patterns within gene expression data, its performance heavily depends on the choice of 'k' and the metric used for measuring similarity, which can be challenging in high-dimensional spaces. The linear nature of linear regression might limit its ability to capture complex relationships in the data unless feature engineering or selection is effectively employed to reduce dimensionality and multicollinearity. All of the three models, random forest, knn and logistic regression are commonly used basic model for classification. I think these methods are appropriate. Appropriate, as here the purpose is for classification of samples. The chosen method are very matched to the purpose given binary strings. the methods are feasible and efficient The logistic regression mentioned is appropriate for gene expression analysis. KNN will be effective for classification. The random forest method is appropriate for determining how the variables impact one another. They seem reasonable. However, for random forest, it is required to find a threshold for each transcript, which might be a problem. In general, I feel like regression methods are better for this task (linear or some combination of features to infer output).\n",
      "After:  random forest is appropriate for its robustness and ability to handle high-dimensional data. knn could be effective in identifying stages based on similarities in gene expression patterns. logistic regression might be useful for its simplicity and interpretability.\n",
      "Before:  accuracy, precision, recall, and F1-score The proposed evaluation metrics, accuracy, precision, recall, and F1-score, are highly appropriate for the project on cancer stage classification using machine learning techniques. They allow for a good assessment of the model's performance, taking into account not only the overall accuracy but also the model's reliability in truly identifying each cancer stage while minimizing false diagnoses. The proposal to use accuracy, precision, recall, and F1-score as evaluation metrics is suitable for classification problems. These metrics will provide a comprehensive view of the model's performance and are standard in machine learning evaluations. The proposal plans to use a comprehensive set of evaluation metrics, including accuracy, precision, recall, F1-score, and cross-validation techniques, which are highly appropriate and standard for assessing the performance of classification models. They include all commonly used evaluation metrices. Traditional method chosen, and is fit for the method. the evalution metrics are correct The evaluation metrics are those we discussed in class and can work properly with the methods they have provided. Cross-validation + accuracy, recall ... (methods mentioned in class) seems reasonable. Although since it is not mentioned in the proposal, I would still suggest using more than one database to avoid overfitting.\n",
      "After:  the proposal to use accuracy, precision, recall, and F1-score as evaluation metrics is suitable for classification problems. these metrics will provide a comprehensive view of the model's performance and are standard in machine learning evaluations.\n",
      "Before:  They are highly related to machine learning analysis of RNA-seq data for diagnostic and prognostic The choose of previous researches are suitable but the description of the paper are highly-concluded (it might be better to discuss what has been investigated in details). The cited works are relevant and support the project's methodology and objectives. They demonstrate an awareness of current research trends in the integration of machine learning with gene expression data for cancer classification. Description of related work is not provided in the proposal. The works in references are related to the proposal plan. Those works revel their efforts in using machine learning methods to make prediction on cancers based on gene expression data. We can see from the titles of those referenced paper they are closely relevant. But they are not discussed or mentioned in the proposal. Citation are added so i suppose the content are included in the begining paragraph. However, it will be good to clearify the content in each article separately as required. there is not enough related works They did not discuss or mention related works. The 3 papers described ML applications for this problem.\n",
      "After:  cited works are relevant and support the project's methodology and objectives. works in references revel their efforts in using machine learning methods to make prediction on cancers based on gene expression data. proposal did not discuss or mention related works.\n",
      "Before:  Very brief The proposal is highly-concise and clear (within 1 or 2 pages as required), which is really good. And this proposal clearly describes the significance and methods of this project, including the data preprocessing approaches, machine learning methods and evaluation metrics. Well-selected machine learning techniques suitable for the complexity of gene expression data. Comprehensive evaluation plan using relevant metrics. he proposal addresses a crucial problem in cancer diagnosis and treatment. Also, the selection comprehensive evaluation metrics demonstrates a thoughtful approach to developing a robust model. It is clear and condensed. Easy to read and understand. Very strictly satisfied the 1-page limit (although it is not strictly required). Very reasonable question to study based on the chosen data sample, and the study shows significance according to the cited article. The method chosen are good fit with the question to study, and more advanced method can be chosen (that we learn in future course) if needed. Clear amd concise explanation for the background and research question, which is good for a Intro. the problem are clearly stated and concise in the given space The proposal appropriately describes the machine learning techniques that they will apply to the dataset. The proposal clearly stated the question and methods for analysis + evaluation.\n",
      "After:  proposal is highly-concise and clear (within 1 or 2 pages as required), which is really good. well-selected machine learning techniques suitable for the complexity of gene expression data. proposal addresses a crucial problem in cancer diagnosis and treatment.\n",
      "Before:  a little not enough detail Although the proposal is highly-concise and comprehensive, it might be better to write more about the details of the machine learning methods, such as how the specific parameters in the models will be selected (just the selection principles and logistics, since it may not be able to decide the specific parameters). Moreover, personally speaking, cancer stage dataset may be likely to be imbalanced, therefore it might be better to mention how such imbalance will be handled in order to ensuring the accuracy of classification. Also, it might be better to mention the ethical considerations and privacy concerns given the use of patient data. I am looking forward to the result of this project and to gain some insights when the findings are interpreted in a clinical context or integrated into existing clinical workflows. A discussion on the potential for model overfitting and how it will be mitigated, particularly with complex models like Random Forest, would be beneficial. The descriptions of the chosen machine learning methods lack detailed implementation strategies. While the proposal mentions preprocessing and PCA for dimensionality reduction, it may underestimate the challenges associated with the high dimensionality and complexity of gene expression data. Additionally, the proposal does not describe any of the related works. Since it is short, it might omit the discussion of the related works and the used method. More details are clearly needed. How the data looks like, and the size and dimension will influence the performamce of the method chosen. How you can determine the standard for each cancer state, such when doing classification you can identify whether the sample belongs to the specific stage. Those are the question I am thinking about but not answered when looking at the proposed idea. a little short for the method and related works They did not discuss or mention related works. Not enough description for the data input and not enough justifications for the methods chosen.\n",
      "After:  the proposal is highly-concise and comprehensive, but it might be better to write more about the details of the machine learning methods. a discussion on the potential for model overfitting and how it will be mitigated would be beneficial. the descriptions of the chosen machine\n"
     ]
    }
   ],
   "source": [
    "summaries = summarize_open_questions(data)\n",
    "export_results(output_path, averages, summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "struct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
